# Configuration for a deeper TransformerInterpNetwork
model_type: "transformer"

model:
  hidden_dim: 192
  num_heads: 8
  num_layers: 2
  dropout: 0.2

training:
  learning_rate: 2e-4
  batch_size: 8
  patience: 10
