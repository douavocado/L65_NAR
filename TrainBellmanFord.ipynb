{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31125ec2-3921-4879-99e1-18059f75d338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-15 11:49:57.906678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739620197.915837   12672 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739620197.918554   12672 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-15 11:49:59.971705: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "INFO:2025-02-15 11:49:59,981:jax._src.xla_bridge:945: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "I0215 11:49:59.981147 131401512719488 xla_bridge.py:945] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:2025-02-15 11:49:59,981:jax._src.xla_bridge:945: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "I0215 11:49:59.981738 131401512719488 xla_bridge.py:945] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "I0215 11:50:00.014345 131401512719488 run.py:335] Creating samplers for algo bellman_ford\n",
      "W0215 11:50:00.014770 131401512719488 samplers.py:341] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>\n",
      "W0215 11:50:00.014956 131401512719488 samplers.py:114] Sampling dataset on-the-fly, unlimited samples.\n",
      "W0215 11:50:00.110707 131401512719488 samplers.py:341] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>\n",
      "W0215 11:50:00.230311 131401512719488 samplers.py:341] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>\n",
      "W0215 11:50:00.406477 131401512719488 samplers.py:341] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>\n",
      "W0215 11:50:00.614046 131401512719488 samplers.py:341] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>\n",
      "W0215 11:50:00.900672 131401512719488 samplers.py:341] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>\n",
      "I0215 11:50:00.900935 131401512719488 samplers.py:130] Creating a dataset with 64 samples.\n",
      "I0215 11:50:00.922824 131401512719488 run.py:156] Dataset found at downloads/CLRS30/CLRS30_v1.0.0. Skipping download.\n",
      "I0215 11:50:00.923563 131401512719488 dataset_info.py:707] Load dataset info from downloads/CLRS30/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0\n",
      "I0215 11:50:00.925724 131401512719488 dataset_info.py:707] Load dataset info from downloads/CLRS30/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0\n",
      "I0215 11:50:00.928801 131401512719488 reader.py:262] Creating a tf.data.Dataset reading 1 files located in folders: downloads/CLRS30/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0.\n",
      "I0215 11:50:00.978072 131401512719488 logging_logger.py:49] Constructing tf.data.Dataset clrs_dataset for split test, from downloads/CLRS30/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0\n",
      "I0215 11:50:05.985414 131401512719488 run.py:519] Algo bellman_ford step 0 current loss 3.840088, current_train_items 32.\n",
      "I0215 11:50:06.499439 131401512719488 run.py:539] (val) algo bellman_ford step 0: {'pi': 0.34375, 'score': 0.34375, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}\n",
      "I0215 11:50:06.499598 131401512719488 run.py:555] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.344, val scores are: bellman_ford: 0.344\n",
      "I0215 11:50:08.897891 131401512719488 run.py:519] Algo bellman_ford step 1 current loss 3.595100, current_train_items 64.\n",
      "I0215 11:50:11.229903 131401512719488 run.py:519] Algo bellman_ford step 2 current loss 3.913796, current_train_items 96.\n",
      "I0215 11:50:14.116433 131401512719488 run.py:519] Algo bellman_ford step 3 current loss 3.398429, current_train_items 128.\n",
      "I0215 11:50:17.196727 131401512719488 run.py:519] Algo bellman_ford step 4 current loss 3.259691, current_train_items 160.\n",
      "I0215 11:50:17.251027 131401512719488 run.py:519] Algo bellman_ford step 5 current loss 1.418120, current_train_items 192.\n",
      "I0215 11:50:17.317497 131401512719488 run.py:519] Algo bellman_ford step 6 current loss 1.794229, current_train_items 224.\n",
      "I0215 11:50:17.479481 131401512719488 run.py:519] Algo bellman_ford step 7 current loss 2.075669, current_train_items 256.\n",
      "I0215 11:50:17.805470 131401512719488 run.py:519] Algo bellman_ford step 8 current loss 2.477528, current_train_items 288.\n",
      "I0215 11:50:18.293940 131401512719488 run.py:519] Algo bellman_ford step 9 current loss 2.279184, current_train_items 320.\n",
      "I0215 11:50:18.344442 131401512719488 run.py:519] Algo bellman_ford step 10 current loss 0.940597, current_train_items 352.\n",
      "I0215 11:50:18.408657 131401512719488 run.py:519] Algo bellman_ford step 11 current loss 1.345648, current_train_items 384.\n",
      "I0215 11:50:18.579626 131401512719488 run.py:519] Algo bellman_ford step 12 current loss 1.668312, current_train_items 416.\n",
      "I0215 11:50:18.916108 131401512719488 run.py:519] Algo bellman_ford step 13 current loss 1.993163, current_train_items 448.\n",
      "I0215 11:50:19.410781 131401512719488 run.py:519] Algo bellman_ford step 14 current loss 2.284345, current_train_items 480.\n",
      "I0215 11:50:19.436536 131401512719488 run.py:519] Algo bellman_ford step 15 current loss 0.746400, current_train_items 512.\n",
      "I0215 11:50:19.504352 131401512719488 run.py:519] Algo bellman_ford step 16 current loss 1.118439, current_train_items 544.\n",
      "I0215 11:50:19.670732 131401512719488 run.py:519] Algo bellman_ford step 17 current loss 1.447175, current_train_items 576.\n",
      "I0215 11:50:20.002500 131401512719488 run.py:519] Algo bellman_ford step 18 current loss 1.772259, current_train_items 608.\n",
      "I0215 11:50:20.498164 131401512719488 run.py:519] Algo bellman_ford step 19 current loss 1.577385, current_train_items 640.\n",
      "I0215 11:50:20.522919 131401512719488 run.py:519] Algo bellman_ford step 20 current loss 0.521280, current_train_items 672.\n",
      "I0215 11:50:20.596204 131401512719488 run.py:519] Algo bellman_ford step 21 current loss 1.095891, current_train_items 704.\n",
      "I0215 11:50:20.759797 131401512719488 run.py:519] Algo bellman_ford step 22 current loss 1.298694, current_train_items 736.\n",
      "I0215 11:50:21.090052 131401512719488 run.py:519] Algo bellman_ford step 23 current loss 1.236035, current_train_items 768.\n",
      "I0215 11:50:21.594793 131401512719488 run.py:519] Algo bellman_ford step 24 current loss 1.700263, current_train_items 800.\n",
      "I0215 11:50:21.618526 131401512719488 run.py:519] Algo bellman_ford step 25 current loss 0.417157, current_train_items 832.\n",
      "I0215 11:50:21.697024 131401512719488 run.py:519] Algo bellman_ford step 26 current loss 0.727380, current_train_items 864.\n",
      "I0215 11:50:21.875802 131401512719488 run.py:519] Algo bellman_ford step 27 current loss 0.871709, current_train_items 896.\n",
      "I0215 11:50:22.208433 131401512719488 run.py:519] Algo bellman_ford step 28 current loss 1.177647, current_train_items 928.\n",
      "I0215 11:50:22.711880 131401512719488 run.py:519] Algo bellman_ford step 29 current loss 1.375258, current_train_items 960.\n",
      "I0215 11:50:22.734863 131401512719488 run.py:519] Algo bellman_ford step 30 current loss 0.323926, current_train_items 992.\n",
      "I0215 11:50:22.805801 131401512719488 run.py:519] Algo bellman_ford step 31 current loss 0.618263, current_train_items 1024.\n",
      "I0215 11:50:22.976460 131401512719488 run.py:519] Algo bellman_ford step 32 current loss 0.813694, current_train_items 1056.\n",
      "I0215 11:50:23.299210 131401512719488 run.py:519] Algo bellman_ford step 33 current loss 1.061140, current_train_items 1088.\n",
      "I0215 11:50:23.801440 131401512719488 run.py:519] Algo bellman_ford step 34 current loss 1.212933, current_train_items 1120.\n",
      "I0215 11:50:23.826121 131401512719488 run.py:519] Algo bellman_ford step 35 current loss 0.259364, current_train_items 1152.\n",
      "I0215 11:50:23.896650 131401512719488 run.py:519] Algo bellman_ford step 36 current loss 0.530012, current_train_items 1184.\n",
      "I0215 11:50:24.063453 131401512719488 run.py:519] Algo bellman_ford step 37 current loss 0.678128, current_train_items 1216.\n",
      "I0215 11:50:24.394880 131401512719488 run.py:519] Algo bellman_ford step 38 current loss 0.847385, current_train_items 1248.\n",
      "I0215 11:50:24.902374 131401512719488 run.py:519] Algo bellman_ford step 39 current loss 0.813408, current_train_items 1280.\n",
      "I0215 11:50:24.927134 131401512719488 run.py:519] Algo bellman_ford step 40 current loss 0.213819, current_train_items 1312.\n",
      "I0215 11:50:25.002149 131401512719488 run.py:519] Algo bellman_ford step 41 current loss 0.355748, current_train_items 1344.\n",
      "I0215 11:50:25.172676 131401512719488 run.py:519] Algo bellman_ford step 42 current loss 0.637462, current_train_items 1376.\n",
      "I0215 11:50:25.511569 131401512719488 run.py:519] Algo bellman_ford step 43 current loss 0.671081, current_train_items 1408.\n",
      "I0215 11:50:26.016135 131401512719488 run.py:519] Algo bellman_ford step 44 current loss 0.757968, current_train_items 1440.\n",
      "I0215 11:50:26.042009 131401512719488 run.py:519] Algo bellman_ford step 45 current loss 0.150901, current_train_items 1472.\n",
      "I0215 11:50:26.115248 131401512719488 run.py:519] Algo bellman_ford step 46 current loss 0.419270, current_train_items 1504.\n",
      "I0215 11:50:26.281622 131401512719488 run.py:519] Algo bellman_ford step 47 current loss 0.507418, current_train_items 1536.\n",
      "I0215 11:50:26.619922 131401512719488 run.py:519] Algo bellman_ford step 48 current loss 0.880215, current_train_items 1568.\n",
      "I0215 11:50:27.123012 131401512719488 run.py:519] Algo bellman_ford step 49 current loss 1.048882, current_train_items 1600.\n",
      "I0215 11:50:27.147027 131401512719488 run.py:519] Algo bellman_ford step 50 current loss 0.145105, current_train_items 1632.\n",
      "I0215 11:50:27.251589 131401512719488 run.py:539] (val) algo bellman_ford step 50: {'pi': 0.8740234375, 'score': 0.8740234375, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}\n",
      "I0215 11:50:27.251744 131401512719488 run.py:555] Checkpointing best model, best avg val score was 0.344, current avg val score is 0.874, val scores are: bellman_ford: 0.874\n",
      "I0215 11:50:27.271787 131401512719488 run.py:519] Algo bellman_ford step 51 current loss 0.401662, current_train_items 1664.\n",
      "I0215 11:50:27.500929 131401512719488 run.py:519] Algo bellman_ford step 52 current loss 0.587076, current_train_items 1696.\n",
      "I0215 11:50:27.833445 131401512719488 run.py:519] Algo bellman_ford step 53 current loss 0.672152, current_train_items 1728.\n",
      "I0215 11:50:28.341598 131401512719488 run.py:519] Algo bellman_ford step 54 current loss 0.741266, current_train_items 1760.\n",
      "I0215 11:50:28.370002 131401512719488 run.py:519] Algo bellman_ford step 55 current loss 0.112268, current_train_items 1792.\n",
      "I0215 11:50:28.450084 131401512719488 run.py:519] Algo bellman_ford step 56 current loss 0.428749, current_train_items 1824.\n",
      "I0215 11:50:28.624882 131401512719488 run.py:519] Algo bellman_ford step 57 current loss 0.662522, current_train_items 1856.\n",
      "I0215 11:50:28.957475 131401512719488 run.py:519] Algo bellman_ford step 58 current loss 0.686859, current_train_items 1888.\n",
      "I0215 11:50:29.463298 131401512719488 run.py:519] Algo bellman_ford step 59 current loss 0.793103, current_train_items 1920.\n",
      "I0215 11:50:29.519962 131401512719488 run.py:519] Algo bellman_ford step 60 current loss 0.190814, current_train_items 1952.\n",
      "I0215 11:50:29.584693 131401512719488 run.py:519] Algo bellman_ford step 61 current loss 0.293737, current_train_items 1984.\n",
      "I0215 11:50:29.758237 131401512719488 run.py:519] Algo bellman_ford step 62 current loss 0.608317, current_train_items 2016.\n",
      "I0215 11:50:30.088036 131401512719488 run.py:519] Algo bellman_ford step 63 current loss 0.592202, current_train_items 2048.\n",
      "I0215 11:50:30.585176 131401512719488 run.py:519] Algo bellman_ford step 64 current loss 0.754843, current_train_items 2080.\n",
      "I0215 11:50:30.613263 131401512719488 run.py:519] Algo bellman_ford step 65 current loss 0.110304, current_train_items 2112.\n",
      "I0215 11:50:30.689873 131401512719488 run.py:519] Algo bellman_ford step 66 current loss 0.234393, current_train_items 2144.\n",
      "I0215 11:50:30.861507 131401512719488 run.py:519] Algo bellman_ford step 67 current loss 0.407690, current_train_items 2176.\n",
      "I0215 11:50:31.196416 131401512719488 run.py:519] Algo bellman_ford step 68 current loss 0.687788, current_train_items 2208.\n",
      "I0215 11:50:31.704873 131401512719488 run.py:519] Algo bellman_ford step 69 current loss 0.832176, current_train_items 2240.\n",
      "I0215 11:50:31.732068 131401512719488 run.py:519] Algo bellman_ford step 70 current loss 0.111909, current_train_items 2272.\n",
      "I0215 11:50:31.804134 131401512719488 run.py:519] Algo bellman_ford step 71 current loss 0.328428, current_train_items 2304.\n",
      "I0215 11:50:31.972250 131401512719488 run.py:519] Algo bellman_ford step 72 current loss 0.524311, current_train_items 2336.\n",
      "I0215 11:50:32.304446 131401512719488 run.py:519] Algo bellman_ford step 73 current loss 0.580295, current_train_items 2368.\n",
      "I0215 11:50:32.815289 131401512719488 run.py:519] Algo bellman_ford step 74 current loss 0.490452, current_train_items 2400.\n",
      "I0215 11:50:32.840533 131401512719488 run.py:519] Algo bellman_ford step 75 current loss 0.131655, current_train_items 2432.\n",
      "I0215 11:50:32.918606 131401512719488 run.py:519] Algo bellman_ford step 76 current loss 0.253724, current_train_items 2464.\n",
      "I0215 11:50:33.088311 131401512719488 run.py:519] Algo bellman_ford step 77 current loss 0.535728, current_train_items 2496.\n",
      "I0215 11:50:33.425988 131401512719488 run.py:519] Algo bellman_ford step 78 current loss 0.385161, current_train_items 2528.\n",
      "I0215 11:50:33.946733 131401512719488 run.py:519] Algo bellman_ford step 79 current loss 0.633672, current_train_items 2560.\n",
      "I0215 11:50:33.972663 131401512719488 run.py:519] Algo bellman_ford step 80 current loss 0.115240, current_train_items 2592.\n",
      "I0215 11:50:34.048470 131401512719488 run.py:519] Algo bellman_ford step 81 current loss 0.333696, current_train_items 2624.\n",
      "I0215 11:50:34.216442 131401512719488 run.py:519] Algo bellman_ford step 82 current loss 0.428592, current_train_items 2656.\n",
      "I0215 11:50:34.544904 131401512719488 run.py:519] Algo bellman_ford step 83 current loss 0.533099, current_train_items 2688.\n",
      "I0215 11:50:35.061787 131401512719488 run.py:519] Algo bellman_ford step 84 current loss 0.479940, current_train_items 2720.\n",
      "I0215 11:50:35.120074 131401512719488 run.py:519] Algo bellman_ford step 85 current loss 0.055649, current_train_items 2752.\n",
      "I0215 11:50:35.185753 131401512719488 run.py:519] Algo bellman_ford step 86 current loss 0.187350, current_train_items 2784.\n",
      "I0215 11:50:35.382508 131401512719488 run.py:519] Algo bellman_ford step 87 current loss 0.383281, current_train_items 2816.\n",
      "I0215 11:50:35.738827 131401512719488 run.py:519] Algo bellman_ford step 88 current loss 0.547541, current_train_items 2848.\n",
      "I0215 11:50:36.246935 131401512719488 run.py:519] Algo bellman_ford step 89 current loss 0.670752, current_train_items 2880.\n",
      "I0215 11:50:36.270298 131401512719488 run.py:519] Algo bellman_ford step 90 current loss 0.096506, current_train_items 2912.\n",
      "I0215 11:50:36.343932 131401512719488 run.py:519] Algo bellman_ford step 91 current loss 0.289329, current_train_items 2944.\n",
      "I0215 11:50:36.507376 131401512719488 run.py:519] Algo bellman_ford step 92 current loss 0.295863, current_train_items 2976.\n",
      "I0215 11:50:36.844868 131401512719488 run.py:519] Algo bellman_ford step 93 current loss 0.472650, current_train_items 3008.\n",
      "I0215 11:50:37.385772 131401512719488 run.py:519] Algo bellman_ford step 94 current loss 0.562323, current_train_items 3040.\n",
      "I0215 11:50:37.411991 131401512719488 run.py:519] Algo bellman_ford step 95 current loss 0.077844, current_train_items 3072.\n",
      "I0215 11:50:37.486262 131401512719488 run.py:519] Algo bellman_ford step 96 current loss 0.222150, current_train_items 3104.\n",
      "W0215 11:50:37.494835 131401512719488 samplers.py:189] Increasing hint lengh from 8 to 9\n",
      "I0215 11:50:40.376136 131401512719488 run.py:519] Algo bellman_ford step 97 current loss 0.417612, current_train_items 3136.\n",
      "I0215 11:50:40.751978 131401512719488 run.py:519] Algo bellman_ford step 98 current loss 0.390164, current_train_items 3168.\n",
      "I0215 11:50:41.369836 131401512719488 run.py:519] Algo bellman_ford step 99 current loss 0.521776, current_train_items 3200.\n",
      "I0215 11:50:41.400193 131401512719488 run.py:519] Algo bellman_ford step 100 current loss 0.126528, current_train_items 3232.\n",
      "I0215 11:50:41.512707 131401512719488 run.py:539] (val) algo bellman_ford step 100: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}\n",
      "I0215 11:50:41.512989 131401512719488 run.py:555] Checkpointing best model, best avg val score was 0.874, current avg val score is 0.901, val scores are: bellman_ford: 0.901\n",
      "I0215 11:50:41.533812 131401512719488 run.py:519] Algo bellman_ford step 101 current loss 0.256830, current_train_items 3264.\n",
      "I0215 11:50:41.815279 131401512719488 run.py:519] Algo bellman_ford step 102 current loss 0.289380, current_train_items 3296.\n",
      "I0215 11:50:42.177629 131401512719488 run.py:519] Algo bellman_ford step 103 current loss 0.308693, current_train_items 3328.\n",
      "I0215 11:50:42.717916 131401512719488 run.py:519] Algo bellman_ford step 104 current loss 0.322731, current_train_items 3360.\n",
      "I0215 11:50:42.746148 131401512719488 run.py:519] Algo bellman_ford step 105 current loss 0.091391, current_train_items 3392.\n",
      "I0215 11:50:42.824423 131401512719488 run.py:519] Algo bellman_ford step 106 current loss 0.158795, current_train_items 3424.\n",
      "I0215 11:50:43.041830 131401512719488 run.py:519] Algo bellman_ford step 107 current loss 0.309481, current_train_items 3456.\n",
      "I0215 11:50:43.413471 131401512719488 run.py:519] Algo bellman_ford step 108 current loss 0.236376, current_train_items 3488.\n",
      "I0215 11:50:43.966632 131401512719488 run.py:519] Algo bellman_ford step 109 current loss 0.381252, current_train_items 3520.\n",
      "I0215 11:50:44.025941 131401512719488 run.py:519] Algo bellman_ford step 110 current loss 0.106358, current_train_items 3552.\n",
      "I0215 11:50:44.094719 131401512719488 run.py:519] Algo bellman_ford step 111 current loss 0.192854, current_train_items 3584.\n",
      "I0215 11:50:44.298796 131401512719488 run.py:519] Algo bellman_ford step 112 current loss 0.430286, current_train_items 3616.\n",
      "I0215 11:50:44.650318 131401512719488 run.py:519] Algo bellman_ford step 113 current loss 0.439827, current_train_items 3648.\n",
      "I0215 11:50:45.186971 131401512719488 run.py:519] Algo bellman_ford step 114 current loss 0.333926, current_train_items 3680.\n",
      "I0215 11:50:45.215310 131401512719488 run.py:519] Algo bellman_ford step 115 current loss 0.042834, current_train_items 3712.\n",
      "I0215 11:50:45.287757 131401512719488 run.py:519] Algo bellman_ford step 116 current loss 0.238220, current_train_items 3744.\n",
      "I0215 11:50:45.491721 131401512719488 run.py:519] Algo bellman_ford step 117 current loss 0.398563, current_train_items 3776.\n",
      "I0215 11:50:45.850202 131401512719488 run.py:519] Algo bellman_ford step 118 current loss 0.486796, current_train_items 3808.\n",
      "I0215 11:50:46.395516 131401512719488 run.py:519] Algo bellman_ford step 119 current loss 0.437603, current_train_items 3840.\n",
      "I0215 11:50:46.452404 131401512719488 run.py:519] Algo bellman_ford step 120 current loss 0.019408, current_train_items 3872.\n",
      "I0215 11:50:46.523797 131401512719488 run.py:519] Algo bellman_ford step 121 current loss 0.133927, current_train_items 3904.\n",
      "I0215 11:50:46.729054 131401512719488 run.py:519] Algo bellman_ford step 122 current loss 0.433971, current_train_items 3936.\n",
      "I0215 11:50:47.092476 131401512719488 run.py:519] Algo bellman_ford step 123 current loss 0.431219, current_train_items 3968.\n",
      "I0215 11:50:47.629872 131401512719488 run.py:519] Algo bellman_ford step 124 current loss 0.399358, current_train_items 4000.\n",
      "I0215 11:50:47.656481 131401512719488 run.py:519] Algo bellman_ford step 125 current loss 0.062067, current_train_items 4032.\n",
      "I0215 11:50:47.733623 131401512719488 run.py:519] Algo bellman_ford step 126 current loss 0.355033, current_train_items 4064.\n",
      "I0215 11:50:47.939313 131401512719488 run.py:519] Algo bellman_ford step 127 current loss 0.341271, current_train_items 4096.\n",
      "I0215 11:50:48.290585 131401512719488 run.py:519] Algo bellman_ford step 128 current loss 0.255290, current_train_items 4128.\n",
      "I0215 11:50:48.862997 131401512719488 run.py:519] Algo bellman_ford step 129 current loss 0.260039, current_train_items 4160.\n",
      "I0215 11:50:48.891377 131401512719488 run.py:519] Algo bellman_ford step 130 current loss 0.100527, current_train_items 4192.\n",
      "I0215 11:50:48.968117 131401512719488 run.py:519] Algo bellman_ford step 131 current loss 0.163356, current_train_items 4224.\n",
      "I0215 11:50:49.168790 131401512719488 run.py:519] Algo bellman_ford step 132 current loss 0.235180, current_train_items 4256.\n",
      "I0215 11:50:49.528000 131401512719488 run.py:519] Algo bellman_ford step 133 current loss 0.391637, current_train_items 4288.\n",
      "I0215 11:50:50.104535 131401512719488 run.py:519] Algo bellman_ford step 134 current loss 0.324440, current_train_items 4320.\n",
      "I0215 11:50:50.131935 131401512719488 run.py:519] Algo bellman_ford step 135 current loss 0.017555, current_train_items 4352.\n",
      "I0215 11:50:50.203570 131401512719488 run.py:519] Algo bellman_ford step 136 current loss 0.171962, current_train_items 4384.\n",
      "I0215 11:50:50.405704 131401512719488 run.py:519] Algo bellman_ford step 137 current loss 0.602563, current_train_items 4416.\n",
      "I0215 11:50:50.768883 131401512719488 run.py:519] Algo bellman_ford step 138 current loss 0.579812, current_train_items 4448.\n",
      "I0215 11:50:51.321817 131401512719488 run.py:519] Algo bellman_ford step 139 current loss 0.477991, current_train_items 4480.\n",
      "I0215 11:50:51.351478 131401512719488 run.py:519] Algo bellman_ford step 140 current loss 0.134002, current_train_items 4512.\n",
      "I0215 11:50:51.429060 131401512719488 run.py:519] Algo bellman_ford step 141 current loss 0.207001, current_train_items 4544.\n",
      "I0215 11:50:51.650215 131401512719488 run.py:519] Algo bellman_ford step 142 current loss 0.203729, current_train_items 4576.\n",
      "I0215 11:50:52.029422 131401512719488 run.py:519] Algo bellman_ford step 143 current loss 0.303224, current_train_items 4608.\n",
      "I0215 11:50:52.586022 131401512719488 run.py:519] Algo bellman_ford step 144 current loss 0.443595, current_train_items 4640.\n",
      "I0215 11:50:52.613829 131401512719488 run.py:519] Algo bellman_ford step 145 current loss 0.022903, current_train_items 4672.\n",
      "I0215 11:50:52.695264 131401512719488 run.py:519] Algo bellman_ford step 146 current loss 0.080272, current_train_items 4704.\n",
      "I0215 11:50:52.903737 131401512719488 run.py:519] Algo bellman_ford step 147 current loss 0.328665, current_train_items 4736.\n",
      "I0215 11:50:53.272343 131401512719488 run.py:519] Algo bellman_ford step 148 current loss 0.300868, current_train_items 4768.\n",
      "I0215 11:50:53.830074 131401512719488 run.py:519] Algo bellman_ford step 149 current loss 0.404202, current_train_items 4800.\n",
      "I0215 11:50:53.857815 131401512719488 run.py:519] Algo bellman_ford step 150 current loss 0.033793, current_train_items 4832.\n",
      "I0215 11:50:53.966343 131401512719488 run.py:539] (val) algo bellman_ford step 150: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}\n",
      "I0215 11:50:53.966496 131401512719488 run.py:555] Checkpointing best model, best avg val score was 0.901, current avg val score is 0.912, val scores are: bellman_ford: 0.912\n",
      "I0215 11:50:54.037867 131401512719488 run.py:519] Algo bellman_ford step 151 current loss 0.134277, current_train_items 4864.\n",
      "I0215 11:50:54.301405 131401512719488 run.py:519] Algo bellman_ford step 152 current loss 0.247300, current_train_items 4896.\n",
      "I0215 11:50:54.664072 131401512719488 run.py:519] Algo bellman_ford step 153 current loss 0.347555, current_train_items 4928.\n",
      "I0215 11:50:55.215969 131401512719488 run.py:519] Algo bellman_ford step 154 current loss 0.267560, current_train_items 4960.\n",
      "I0215 11:50:55.244088 131401512719488 run.py:519] Algo bellman_ford step 155 current loss 0.161530, current_train_items 4992.\n",
      "I0215 11:50:55.324802 131401512719488 run.py:519] Algo bellman_ford step 156 current loss 0.144138, current_train_items 5024.\n",
      "I0215 11:50:55.535753 131401512719488 run.py:519] Algo bellman_ford step 157 current loss 0.276688, current_train_items 5056.\n",
      "I0215 11:50:55.911408 131401512719488 run.py:519] Algo bellman_ford step 158 current loss 0.351288, current_train_items 5088.\n",
      "I0215 11:50:56.475555 131401512719488 run.py:519] Algo bellman_ford step 159 current loss 0.277796, current_train_items 5120.\n",
      "I0215 11:50:56.539331 131401512719488 run.py:519] Algo bellman_ford step 160 current loss 0.052755, current_train_items 5152.\n",
      "I0215 11:50:56.610030 131401512719488 run.py:519] Algo bellman_ford step 161 current loss 0.117246, current_train_items 5184.\n",
      "I0215 11:50:56.822361 131401512719488 run.py:519] Algo bellman_ford step 162 current loss 0.195509, current_train_items 5216.\n",
      "I0215 11:50:57.180937 131401512719488 run.py:519] Algo bellman_ford step 163 current loss 0.336898, current_train_items 5248.\n",
      "I0215 11:50:57.735562 131401512719488 run.py:519] Algo bellman_ford step 164 current loss 0.414795, current_train_items 5280.\n",
      "I0215 11:50:57.762552 131401512719488 run.py:519] Algo bellman_ford step 165 current loss 0.044466, current_train_items 5312.\n",
      "I0215 11:50:57.846759 131401512719488 run.py:519] Algo bellman_ford step 166 current loss 0.087683, current_train_items 5344.\n",
      "I0215 11:50:58.051338 131401512719488 run.py:519] Algo bellman_ford step 167 current loss 0.236363, current_train_items 5376.\n",
      "I0215 11:50:58.404623 131401512719488 run.py:519] Algo bellman_ford step 168 current loss 0.235898, current_train_items 5408.\n",
      "I0215 11:50:58.956729 131401512719488 run.py:519] Algo bellman_ford step 169 current loss 0.295801, current_train_items 5440.\n",
      "I0215 11:50:59.015840 131401512719488 run.py:519] Algo bellman_ford step 170 current loss 0.070478, current_train_items 5472.\n",
      "I0215 11:50:59.083694 131401512719488 run.py:519] Algo bellman_ford step 171 current loss 0.146720, current_train_items 5504.\n",
      "I0215 11:50:59.286570 131401512719488 run.py:519] Algo bellman_ford step 172 current loss 0.309930, current_train_items 5536.\n",
      "I0215 11:50:59.646079 131401512719488 run.py:519] Algo bellman_ford step 173 current loss 0.241276, current_train_items 5568.\n",
      "I0215 11:51:00.209463 131401512719488 run.py:519] Algo bellman_ford step 174 current loss 0.481593, current_train_items 5600.\n",
      "I0215 11:51:00.246180 131401512719488 run.py:519] Algo bellman_ford step 175 current loss 0.023133, current_train_items 5632.\n",
      "I0215 11:51:00.329592 131401512719488 run.py:519] Algo bellman_ford step 176 current loss 0.180083, current_train_items 5664.\n",
      "I0215 11:51:00.536262 131401512719488 run.py:519] Algo bellman_ford step 177 current loss 0.296108, current_train_items 5696.\n",
      "I0215 11:51:00.894355 131401512719488 run.py:519] Algo bellman_ford step 178 current loss 0.247894, current_train_items 5728.\n",
      "I0215 11:51:01.449640 131401512719488 run.py:519] Algo bellman_ford step 179 current loss 0.315502, current_train_items 5760.\n",
      "I0215 11:51:01.512137 131401512719488 run.py:519] Algo bellman_ford step 180 current loss 0.026368, current_train_items 5792.\n",
      "I0215 11:51:01.580702 131401512719488 run.py:519] Algo bellman_ford step 181 current loss 0.146178, current_train_items 5824.\n",
      "I0215 11:51:01.789852 131401512719488 run.py:519] Algo bellman_ford step 182 current loss 0.223070, current_train_items 5856.\n",
      "I0215 11:51:02.151495 131401512719488 run.py:519] Algo bellman_ford step 183 current loss 0.226153, current_train_items 5888.\n",
      "I0215 11:51:02.708421 131401512719488 run.py:519] Algo bellman_ford step 184 current loss 0.324172, current_train_items 5920.\n",
      "I0215 11:51:02.735111 131401512719488 run.py:519] Algo bellman_ford step 185 current loss 0.046914, current_train_items 5952.\n",
      "I0215 11:51:02.816635 131401512719488 run.py:519] Algo bellman_ford step 186 current loss 0.089552, current_train_items 5984.\n",
      "I0215 11:51:03.021681 131401512719488 run.py:519] Algo bellman_ford step 187 current loss 0.200568, current_train_items 6016.\n",
      "I0215 11:51:03.373573 131401512719488 run.py:519] Algo bellman_ford step 188 current loss 0.285598, current_train_items 6048.\n",
      "I0215 11:51:03.930522 131401512719488 run.py:519] Algo bellman_ford step 189 current loss 0.456939, current_train_items 6080.\n",
      "I0215 11:51:03.959087 131401512719488 run.py:519] Algo bellman_ford step 190 current loss 0.042567, current_train_items 6112.\n",
      "I0215 11:51:04.038312 131401512719488 run.py:519] Algo bellman_ford step 191 current loss 0.117638, current_train_items 6144.\n",
      "I0215 11:51:04.243545 131401512719488 run.py:519] Algo bellman_ford step 192 current loss 0.239971, current_train_items 6176.\n",
      "I0215 11:51:04.605644 131401512719488 run.py:519] Algo bellman_ford step 193 current loss 0.199117, current_train_items 6208.\n",
      "I0215 11:51:05.161336 131401512719488 run.py:519] Algo bellman_ford step 194 current loss 0.428713, current_train_items 6240.\n",
      "I0215 11:51:05.189895 131401512719488 run.py:519] Algo bellman_ford step 195 current loss 0.066773, current_train_items 6272.\n",
      "I0215 11:51:05.269809 131401512719488 run.py:519] Algo bellman_ford step 196 current loss 0.244240, current_train_items 6304.\n",
      "I0215 11:51:05.478222 131401512719488 run.py:519] Algo bellman_ford step 197 current loss 0.210795, current_train_items 6336.\n",
      "I0215 11:51:05.862022 131401512719488 run.py:519] Algo bellman_ford step 198 current loss 0.175797, current_train_items 6368.\n",
      "I0215 11:51:06.421434 131401512719488 run.py:519] Algo bellman_ford step 199 current loss 0.267454, current_train_items 6400.\n",
      "I0215 11:51:06.448913 131401512719488 run.py:519] Algo bellman_ford step 200 current loss 0.063516, current_train_items 6432.\n",
      "I0215 11:51:06.560293 131401512719488 run.py:539] (val) algo bellman_ford step 200: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}\n",
      "I0215 11:51:06.560456 131401512719488 run.py:558] Not saving new best model, best avg val score was 0.912, current avg val score is 0.912, val scores are: bellman_ford: 0.912\n",
      "I0215 11:51:06.633420 131401512719488 run.py:519] Algo bellman_ford step 201 current loss 0.155431, current_train_items 6464.\n",
      "I0215 11:51:06.846538 131401512719488 run.py:519] Algo bellman_ford step 202 current loss 0.145192, current_train_items 6496.\n",
      "I0215 11:51:07.228483 131401512719488 run.py:519] Algo bellman_ford step 203 current loss 0.310107, current_train_items 6528.\n",
      "I0215 11:51:07.816112 131401512719488 run.py:519] Algo bellman_ford step 204 current loss 0.333672, current_train_items 6560.\n",
      "I0215 11:51:07.847372 131401512719488 run.py:519] Algo bellman_ford step 205 current loss 0.071808, current_train_items 6592.\n",
      "I0215 11:51:07.939375 131401512719488 run.py:519] Algo bellman_ford step 206 current loss 0.065222, current_train_items 6624.\n",
      "I0215 11:51:08.165229 131401512719488 run.py:519] Algo bellman_ford step 207 current loss 0.123980, current_train_items 6656.\n",
      "I0215 11:51:08.546956 131401512719488 run.py:519] Algo bellman_ford step 208 current loss 0.162087, current_train_items 6688.\n",
      "I0215 11:51:09.138817 131401512719488 run.py:519] Algo bellman_ford step 209 current loss 0.186795, current_train_items 6720.\n",
      "I0215 11:51:09.166891 131401512719488 run.py:519] Algo bellman_ford step 210 current loss 0.035480, current_train_items 6752.\n",
      "I0215 11:51:09.248078 131401512719488 run.py:519] Algo bellman_ford step 211 current loss 0.070652, current_train_items 6784.\n",
      "I0215 11:51:09.470784 131401512719488 run.py:519] Algo bellman_ford step 212 current loss 0.163240, current_train_items 6816.\n",
      "I0215 11:51:09.875021 131401512719488 run.py:519] Algo bellman_ford step 213 current loss 0.230326, current_train_items 6848.\n",
      "I0215 11:51:10.475056 131401512719488 run.py:519] Algo bellman_ford step 214 current loss 0.267204, current_train_items 6880.\n",
      "I0215 11:51:10.503884 131401512719488 run.py:519] Algo bellman_ford step 215 current loss 0.022810, current_train_items 6912.\n",
      "I0215 11:51:10.584067 131401512719488 run.py:519] Algo bellman_ford step 216 current loss 0.081918, current_train_items 6944.\n",
      "I0215 11:51:10.800145 131401512719488 run.py:519] Algo bellman_ford step 217 current loss 0.247614, current_train_items 6976.\n",
      "I0215 11:51:11.170900 131401512719488 run.py:519] Algo bellman_ford step 218 current loss 0.235585, current_train_items 7008.\n",
      "I0215 11:51:11.729877 131401512719488 run.py:519] Algo bellman_ford step 219 current loss 0.286327, current_train_items 7040.\n",
      "I0215 11:51:11.756787 131401512719488 run.py:519] Algo bellman_ford step 220 current loss 0.053944, current_train_items 7072.\n",
      "I0215 11:51:11.836446 131401512719488 run.py:519] Algo bellman_ford step 221 current loss 0.080587, current_train_items 7104.\n",
      "I0215 11:51:12.043057 131401512719488 run.py:519] Algo bellman_ford step 222 current loss 0.134528, current_train_items 7136.\n",
      "I0215 11:51:12.397032 131401512719488 run.py:519] Algo bellman_ford step 223 current loss 0.215866, current_train_items 7168.\n",
      "I0215 11:51:12.971740 131401512719488 run.py:519] Algo bellman_ford step 224 current loss 0.220170, current_train_items 7200.\n",
      "I0215 11:51:13.000191 131401512719488 run.py:519] Algo bellman_ford step 225 current loss 0.019226, current_train_items 7232.\n",
      "I0215 11:51:13.091224 131401512719488 run.py:519] Algo bellman_ford step 226 current loss 0.083519, current_train_items 7264.\n",
      "I0215 11:51:13.330702 131401512719488 run.py:519] Algo bellman_ford step 227 current loss 0.058888, current_train_items 7296.\n",
      "I0215 11:51:13.714905 131401512719488 run.py:519] Algo bellman_ford step 228 current loss 0.189526, current_train_items 7328.\n",
      "I0215 11:51:14.295429 131401512719488 run.py:519] Algo bellman_ford step 229 current loss 0.197126, current_train_items 7360.\n",
      "I0215 11:51:14.321736 131401512719488 run.py:519] Algo bellman_ford step 230 current loss 0.004716, current_train_items 7392.\n",
      "I0215 11:51:14.406777 131401512719488 run.py:519] Algo bellman_ford step 231 current loss 0.063493, current_train_items 7424.\n",
      "I0215 11:51:14.612920 131401512719488 run.py:519] Algo bellman_ford step 232 current loss 0.160681, current_train_items 7456.\n",
      "I0215 11:51:15.013315 131401512719488 run.py:519] Algo bellman_ford step 233 current loss 0.216719, current_train_items 7488.\n",
      "I0215 11:51:15.624640 131401512719488 run.py:519] Algo bellman_ford step 234 current loss 0.188893, current_train_items 7520.\n",
      "I0215 11:51:15.653498 131401512719488 run.py:519] Algo bellman_ford step 235 current loss 0.047640, current_train_items 7552.\n",
      "I0215 11:51:15.735550 131401512719488 run.py:519] Algo bellman_ford step 236 current loss 0.189890, current_train_items 7584.\n",
      "I0215 11:51:15.952120 131401512719488 run.py:519] Algo bellman_ford step 237 current loss 0.204261, current_train_items 7616.\n",
      "I0215 11:51:16.320695 131401512719488 run.py:519] Algo bellman_ford step 238 current loss 0.269050, current_train_items 7648.\n",
      "I0215 11:51:16.884976 131401512719488 run.py:519] Algo bellman_ford step 239 current loss 0.252611, current_train_items 7680.\n",
      "I0215 11:51:16.912558 131401512719488 run.py:519] Algo bellman_ford step 240 current loss 0.124537, current_train_items 7712.\n",
      "I0215 11:51:16.995937 131401512719488 run.py:519] Algo bellman_ford step 241 current loss 0.292256, current_train_items 7744.\n",
      "I0215 11:51:17.224546 131401512719488 run.py:519] Algo bellman_ford step 242 current loss 0.330348, current_train_items 7776.\n",
      "I0215 11:51:17.596703 131401512719488 run.py:519] Algo bellman_ford step 243 current loss 0.277346, current_train_items 7808.\n",
      "I0215 11:51:18.166240 131401512719488 run.py:519] Algo bellman_ford step 244 current loss 0.464415, current_train_items 7840.\n",
      "I0215 11:51:18.194008 131401512719488 run.py:519] Algo bellman_ford step 245 current loss 0.049474, current_train_items 7872.\n",
      "I0215 11:51:18.276890 131401512719488 run.py:519] Algo bellman_ford step 246 current loss 0.128069, current_train_items 7904.\n",
      "I0215 11:51:18.477059 131401512719488 run.py:519] Algo bellman_ford step 247 current loss 0.188554, current_train_items 7936.\n",
      "I0215 11:51:18.832782 131401512719488 run.py:519] Algo bellman_ford step 248 current loss 0.253724, current_train_items 7968.\n",
      "I0215 11:51:19.374238 131401512719488 run.py:519] Algo bellman_ford step 249 current loss 0.399864, current_train_items 8000.\n",
      "I0215 11:51:19.399707 131401512719488 run.py:519] Algo bellman_ford step 250 current loss 0.040019, current_train_items 8032.\n",
      "I0215 11:51:19.515219 131401512719488 run.py:539] (val) algo bellman_ford step 250: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}\n",
      "I0215 11:51:19.515369 131401512719488 run.py:555] Checkpointing best model, best avg val score was 0.912, current avg val score is 0.934, val scores are: bellman_ford: 0.934\n",
      "I0215 11:51:19.547108 131401512719488 run.py:519] Algo bellman_ford step 251 current loss 0.126611, current_train_items 8064.\n",
      "I0215 11:51:19.810321 131401512719488 run.py:519] Algo bellman_ford step 252 current loss 0.149110, current_train_items 8096.\n",
      "I0215 11:51:20.152704 131401512719488 run.py:519] Algo bellman_ford step 253 current loss 0.172880, current_train_items 8128.\n",
      "I0215 11:51:20.681781 131401512719488 run.py:519] Algo bellman_ford step 254 current loss 0.317038, current_train_items 8160.\n",
      "I0215 11:51:20.707945 131401512719488 run.py:519] Algo bellman_ford step 255 current loss 0.005029, current_train_items 8192.\n",
      "I0215 11:51:20.799997 131401512719488 run.py:519] Algo bellman_ford step 256 current loss 0.068842, current_train_items 8224.\n",
      "I0215 11:51:21.001218 131401512719488 run.py:519] Algo bellman_ford step 257 current loss 0.165148, current_train_items 8256.\n",
      "I0215 11:51:21.352439 131401512719488 run.py:519] Algo bellman_ford step 258 current loss 0.234986, current_train_items 8288.\n",
      "I0215 11:51:21.864501 131401512719488 run.py:519] Algo bellman_ford step 259 current loss 0.231225, current_train_items 8320.\n",
      "I0215 11:51:21.891590 131401512719488 run.py:519] Algo bellman_ford step 260 current loss 0.040287, current_train_items 8352.\n",
      "I0215 11:51:21.972334 131401512719488 run.py:519] Algo bellman_ford step 261 current loss 0.059619, current_train_items 8384.\n",
      "I0215 11:51:22.172920 131401512719488 run.py:519] Algo bellman_ford step 262 current loss 0.189088, current_train_items 8416.\n",
      "I0215 11:51:22.519578 131401512719488 run.py:519] Algo bellman_ford step 263 current loss 0.174090, current_train_items 8448.\n",
      "I0215 11:51:23.053518 131401512719488 run.py:519] Algo bellman_ford step 264 current loss 0.219740, current_train_items 8480.\n",
      "I0215 11:51:23.079394 131401512719488 run.py:519] Algo bellman_ford step 265 current loss 0.075975, current_train_items 8512.\n",
      "I0215 11:51:23.164760 131401512719488 run.py:519] Algo bellman_ford step 266 current loss 0.207287, current_train_items 8544.\n",
      "I0215 11:51:23.363333 131401512719488 run.py:519] Algo bellman_ford step 267 current loss 0.220385, current_train_items 8576.\n",
      "I0215 11:51:23.707754 131401512719488 run.py:519] Algo bellman_ford step 268 current loss 0.132539, current_train_items 8608.\n",
      "I0215 11:51:24.222862 131401512719488 run.py:519] Algo bellman_ford step 269 current loss 0.257620, current_train_items 8640.\n",
      "I0215 11:51:24.249939 131401512719488 run.py:519] Algo bellman_ford step 270 current loss 0.023704, current_train_items 8672.\n",
      "I0215 11:51:24.332386 131401512719488 run.py:519] Algo bellman_ford step 271 current loss 0.371872, current_train_items 8704.\n",
      "I0215 11:51:24.534985 131401512719488 run.py:519] Algo bellman_ford step 272 current loss 0.357124, current_train_items 8736.\n",
      "I0215 11:51:24.885176 131401512719488 run.py:519] Algo bellman_ford step 273 current loss 0.269139, current_train_items 8768.\n",
      "I0215 11:51:25.415115 131401512719488 run.py:519] Algo bellman_ford step 274 current loss 0.192529, current_train_items 8800.\n",
      "I0215 11:51:25.439608 131401512719488 run.py:519] Algo bellman_ford step 275 current loss 0.021177, current_train_items 8832.\n",
      "I0215 11:51:25.525618 131401512719488 run.py:519] Algo bellman_ford step 276 current loss 0.231518, current_train_items 8864.\n",
      "I0215 11:51:25.737562 131401512719488 run.py:519] Algo bellman_ford step 277 current loss 0.325463, current_train_items 8896.\n",
      "I0215 11:51:26.080571 131401512719488 run.py:519] Algo bellman_ford step 278 current loss 0.283720, current_train_items 8928.\n",
      "I0215 11:51:26.601901 131401512719488 run.py:519] Algo bellman_ford step 279 current loss 0.199919, current_train_items 8960.\n",
      "I0215 11:51:26.627147 131401512719488 run.py:519] Algo bellman_ford step 280 current loss 0.114502, current_train_items 8992.\n",
      "I0215 11:51:26.708930 131401512719488 run.py:519] Algo bellman_ford step 281 current loss 0.184654, current_train_items 9024.\n",
      "I0215 11:51:26.906271 131401512719488 run.py:519] Algo bellman_ford step 282 current loss 0.569343, current_train_items 9056.\n",
      "I0215 11:51:27.255050 131401512719488 run.py:519] Algo bellman_ford step 283 current loss 0.678039, current_train_items 9088.\n",
      "I0215 11:51:27.788417 131401512719488 run.py:519] Algo bellman_ford step 284 current loss 0.651765, current_train_items 9120.\n",
      "I0215 11:51:27.812836 131401512719488 run.py:519] Algo bellman_ford step 285 current loss 0.058910, current_train_items 9152.\n",
      "W0215 11:51:27.818954 131401512719488 samplers.py:189] Increasing hint lengh from 6 to 7\n",
      "I0215 11:51:30.527356 131401512719488 run.py:519] Algo bellman_ford step 286 current loss 0.169433, current_train_items 9184.\n",
      "I0215 11:51:30.719954 131401512719488 run.py:519] Algo bellman_ford step 287 current loss 0.483683, current_train_items 9216.\n",
      "I0215 11:51:31.057164 131401512719488 run.py:519] Algo bellman_ford step 288 current loss 0.374655, current_train_items 9248.\n",
      "I0215 11:51:31.573571 131401512719488 run.py:519] Algo bellman_ford step 289 current loss 0.318225, current_train_items 9280.\n",
      "I0215 11:51:31.599294 131401512719488 run.py:519] Algo bellman_ford step 290 current loss 0.009322, current_train_items 9312.\n",
      "I0215 11:51:31.686811 131401512719488 run.py:519] Algo bellman_ford step 291 current loss 0.059652, current_train_items 9344.\n",
      "I0215 11:51:31.878504 131401512719488 run.py:519] Algo bellman_ford step 292 current loss 0.181687, current_train_items 9376.\n",
      "I0215 11:51:32.221288 131401512719488 run.py:519] Algo bellman_ford step 293 current loss 0.268050, current_train_items 9408.\n",
      "I0215 11:51:32.725551 131401512719488 run.py:519] Algo bellman_ford step 294 current loss 0.217270, current_train_items 9440.\n",
      "I0215 11:51:32.749794 131401512719488 run.py:519] Algo bellman_ford step 295 current loss 0.012777, current_train_items 9472.\n",
      "I0215 11:51:32.840610 131401512719488 run.py:519] Algo bellman_ford step 296 current loss 0.179394, current_train_items 9504.\n",
      "I0215 11:51:33.042251 131401512719488 run.py:519] Algo bellman_ford step 297 current loss 0.301429, current_train_items 9536.\n",
      "I0215 11:51:33.380720 131401512719488 run.py:519] Algo bellman_ford step 298 current loss 0.293880, current_train_items 9568.\n",
      "I0215 11:51:33.899721 131401512719488 run.py:519] Algo bellman_ford step 299 current loss 0.276559, current_train_items 9600.\n",
      "I0215 11:51:33.923988 131401512719488 run.py:519] Algo bellman_ford step 300 current loss 0.027035, current_train_items 9632.\n",
      "I0215 11:51:34.026278 131401512719488 run.py:539] (val) algo bellman_ford step 300: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}\n",
      "I0215 11:51:34.026432 131401512719488 run.py:555] Checkpointing best model, best avg val score was 0.934, current avg val score is 0.938, val scores are: bellman_ford: 0.938\n",
      "I0215 11:51:34.056543 131401512719488 run.py:519] Algo bellman_ford step 301 current loss 0.186066, current_train_items 9664.\n",
      "I0215 11:51:34.311778 131401512719488 run.py:519] Algo bellman_ford step 302 current loss 0.297601, current_train_items 9696.\n",
      "I0215 11:51:34.645950 131401512719488 run.py:519] Algo bellman_ford step 303 current loss 0.425915, current_train_items 9728.\n",
      "I0215 11:51:35.148791 131401512719488 run.py:519] Algo bellman_ford step 304 current loss 0.208139, current_train_items 9760.\n",
      "I0215 11:51:35.174355 131401512719488 run.py:519] Algo bellman_ford step 305 current loss 0.010716, current_train_items 9792.\n",
      "I0215 11:51:35.263699 131401512719488 run.py:519] Algo bellman_ford step 306 current loss 0.175608, current_train_items 9824.\n",
      "I0215 11:51:35.456167 131401512719488 run.py:519] Algo bellman_ford step 307 current loss 0.361861, current_train_items 9856.\n",
      "I0215 11:51:35.793950 131401512719488 run.py:519] Algo bellman_ford step 308 current loss 0.320746, current_train_items 9888.\n",
      "I0215 11:51:36.316949 131401512719488 run.py:519] Algo bellman_ford step 309 current loss 0.335308, current_train_items 9920.\n",
      "I0215 11:51:36.340394 131401512719488 run.py:519] Algo bellman_ford step 310 current loss 0.020119, current_train_items 9952.\n",
      "I0215 11:51:36.433596 131401512719488 run.py:519] Algo bellman_ford step 311 current loss 0.105096, current_train_items 9984.\n",
      "I0215 11:51:36.634072 131401512719488 run.py:519] Algo bellman_ford step 312 current loss 0.186371, current_train_items 10016.\n",
      "I0215 11:51:36.975636 131401512719488 run.py:519] Algo bellman_ford step 313 current loss 0.257665, current_train_items 10048.\n",
      "I0215 11:51:37.500931 131401512719488 run.py:519] Algo bellman_ford step 314 current loss 0.212684, current_train_items 10080.\n",
      "I0215 11:51:37.524866 131401512719488 run.py:519] Algo bellman_ford step 315 current loss 0.016005, current_train_items 10112.\n",
      "I0215 11:51:37.615164 131401512719488 run.py:519] Algo bellman_ford step 316 current loss 0.056714, current_train_items 10144.\n",
      "I0215 11:51:37.811068 131401512719488 run.py:519] Algo bellman_ford step 317 current loss 0.179415, current_train_items 10176.\n",
      "I0215 11:51:38.150722 131401512719488 run.py:519] Algo bellman_ford step 318 current loss 0.095220, current_train_items 10208.\n",
      "I0215 11:51:38.666167 131401512719488 run.py:519] Algo bellman_ford step 319 current loss 0.210989, current_train_items 10240.\n",
      "I0215 11:51:38.690043 131401512719488 run.py:519] Algo bellman_ford step 320 current loss 0.066682, current_train_items 10272.\n",
      "I0215 11:51:38.792105 131401512719488 run.py:519] Algo bellman_ford step 321 current loss 0.109478, current_train_items 10304.\n",
      "I0215 11:51:38.989711 131401512719488 run.py:519] Algo bellman_ford step 322 current loss 0.188842, current_train_items 10336.\n",
      "I0215 11:51:39.334130 131401512719488 run.py:519] Algo bellman_ford step 323 current loss 0.154536, current_train_items 10368.\n",
      "I0215 11:51:39.852398 131401512719488 run.py:519] Algo bellman_ford step 324 current loss 0.210325, current_train_items 10400.\n",
      "I0215 11:51:39.876911 131401512719488 run.py:519] Algo bellman_ford step 325 current loss 0.033373, current_train_items 10432.\n",
      "I0215 11:51:39.963233 131401512719488 run.py:519] Algo bellman_ford step 326 current loss 0.174368, current_train_items 10464.\n",
      "I0215 11:51:40.154217 131401512719488 run.py:519] Algo bellman_ford step 327 current loss 0.075469, current_train_items 10496.\n",
      "I0215 11:51:40.491810 131401512719488 run.py:519] Algo bellman_ford step 328 current loss 0.226677, current_train_items 10528.\n",
      "I0215 11:51:41.047097 131401512719488 run.py:519] Algo bellman_ford step 329 current loss 0.216297, current_train_items 10560.\n",
      "I0215 11:51:41.074971 131401512719488 run.py:519] Algo bellman_ford step 330 current loss 0.007620, current_train_items 10592.\n",
      "I0215 11:51:41.167090 131401512719488 run.py:519] Algo bellman_ford step 331 current loss 0.104666, current_train_items 10624.\n",
      "I0215 11:51:41.392057 131401512719488 run.py:519] Algo bellman_ford step 332 current loss 0.216416, current_train_items 10656.\n",
      "I0215 11:51:41.761564 131401512719488 run.py:519] Algo bellman_ford step 333 current loss 0.273494, current_train_items 10688.\n",
      "I0215 11:51:42.302422 131401512719488 run.py:519] Algo bellman_ford step 334 current loss 0.155157, current_train_items 10720.\n",
      "I0215 11:51:42.328341 131401512719488 run.py:519] Algo bellman_ford step 335 current loss 0.034384, current_train_items 10752.\n",
      "I0215 11:51:42.415204 131401512719488 run.py:519] Algo bellman_ford step 336 current loss 0.097694, current_train_items 10784.\n",
      "I0215 11:51:42.616033 131401512719488 run.py:519] Algo bellman_ford step 337 current loss 0.221078, current_train_items 10816.\n",
      "I0215 11:51:42.959171 131401512719488 run.py:519] Algo bellman_ford step 338 current loss 0.149945, current_train_items 10848.\n",
      "I0215 11:51:43.487346 131401512719488 run.py:519] Algo bellman_ford step 339 current loss 0.174196, current_train_items 10880.\n",
      "I0215 11:51:43.512842 131401512719488 run.py:519] Algo bellman_ford step 340 current loss 0.059516, current_train_items 10912.\n",
      "I0215 11:51:43.599617 131401512719488 run.py:519] Algo bellman_ford step 341 current loss 0.057247, current_train_items 10944.\n",
      "I0215 11:51:43.810580 131401512719488 run.py:519] Algo bellman_ford step 342 current loss 0.113644, current_train_items 10976.\n",
      "I0215 11:51:44.168225 131401512719488 run.py:519] Algo bellman_ford step 343 current loss 0.157830, current_train_items 11008.\n",
      "I0215 11:51:44.684909 131401512719488 run.py:519] Algo bellman_ford step 344 current loss 0.126130, current_train_items 11040.\n",
      "I0215 11:51:44.711130 131401512719488 run.py:519] Algo bellman_ford step 345 current loss 0.010237, current_train_items 11072.\n",
      "I0215 11:51:44.798481 131401512719488 run.py:519] Algo bellman_ford step 346 current loss 0.058516, current_train_items 11104.\n",
      "I0215 11:51:44.998024 131401512719488 run.py:519] Algo bellman_ford step 347 current loss 0.109222, current_train_items 11136.\n",
      "I0215 11:51:45.342338 131401512719488 run.py:519] Algo bellman_ford step 348 current loss 0.130597, current_train_items 11168.\n",
      "I0215 11:51:45.867613 131401512719488 run.py:519] Algo bellman_ford step 349 current loss 0.148641, current_train_items 11200.\n",
      "I0215 11:51:45.893457 131401512719488 run.py:519] Algo bellman_ford step 350 current loss 0.009042, current_train_items 11232.\n",
      "I0215 11:51:45.996062 131401512719488 run.py:539] (val) algo bellman_ford step 350: {'pi': 0.9580078125, 'score': 0.9580078125, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}\n",
      "I0215 11:51:45.996224 131401512719488 run.py:555] Checkpointing best model, best avg val score was 0.938, current avg val score is 0.958, val scores are: bellman_ford: 0.958\n",
      "I0215 11:51:46.028887 131401512719488 run.py:519] Algo bellman_ford step 351 current loss 0.075313, current_train_items 11264.\n",
      "I0215 11:51:46.292150 131401512719488 run.py:519] Algo bellman_ford step 352 current loss 0.201505, current_train_items 11296.\n",
      "I0215 11:51:46.632368 131401512719488 run.py:519] Algo bellman_ford step 353 current loss 0.141313, current_train_items 11328.\n",
      "I0215 11:51:47.152772 131401512719488 run.py:519] Algo bellman_ford step 354 current loss 0.241347, current_train_items 11360.\n",
      "I0215 11:51:47.178289 131401512719488 run.py:519] Algo bellman_ford step 355 current loss 0.006859, current_train_items 11392.\n",
      "I0215 11:51:47.270953 131401512719488 run.py:519] Algo bellman_ford step 356 current loss 0.081879, current_train_items 11424.\n",
      "I0215 11:51:47.481395 131401512719488 run.py:519] Algo bellman_ford step 357 current loss 0.205243, current_train_items 11456.\n",
      "I0215 11:51:47.832045 131401512719488 run.py:519] Algo bellman_ford step 358 current loss 0.201064, current_train_items 11488.\n",
      "I0215 11:51:48.361292 131401512719488 run.py:519] Algo bellman_ford step 359 current loss 0.163015, current_train_items 11520.\n",
      "I0215 11:51:48.387319 131401512719488 run.py:519] Algo bellman_ford step 360 current loss 0.037831, current_train_items 11552.\n",
      "I0215 11:51:48.477104 131401512719488 run.py:519] Algo bellman_ford step 361 current loss 0.078694, current_train_items 11584.\n",
      "I0215 11:51:48.673862 131401512719488 run.py:519] Algo bellman_ford step 362 current loss 0.138034, current_train_items 11616.\n",
      "I0215 11:51:49.022935 131401512719488 run.py:519] Algo bellman_ford step 363 current loss 0.150525, current_train_items 11648.\n",
      "I0215 11:51:49.557368 131401512719488 run.py:519] Algo bellman_ford step 364 current loss 0.142183, current_train_items 11680.\n",
      "I0215 11:51:49.582625 131401512719488 run.py:519] Algo bellman_ford step 365 current loss 0.024342, current_train_items 11712.\n",
      "I0215 11:51:49.673290 131401512719488 run.py:519] Algo bellman_ford step 366 current loss 0.033162, current_train_items 11744.\n",
      "I0215 11:51:49.870403 131401512719488 run.py:519] Algo bellman_ford step 367 current loss 0.082630, current_train_items 11776.\n",
      "I0215 11:51:50.216670 131401512719488 run.py:519] Algo bellman_ford step 368 current loss 0.154196, current_train_items 11808.\n",
      "I0215 11:51:50.749778 131401512719488 run.py:519] Algo bellman_ford step 369 current loss 0.208788, current_train_items 11840.\n",
      "I0215 11:51:50.774215 131401512719488 run.py:519] Algo bellman_ford step 370 current loss 0.011125, current_train_items 11872.\n",
      "I0215 11:51:50.864726 131401512719488 run.py:519] Algo bellman_ford step 371 current loss 0.057995, current_train_items 11904.\n",
      "I0215 11:51:51.060906 131401512719488 run.py:519] Algo bellman_ford step 372 current loss 0.088498, current_train_items 11936.\n",
      "I0215 11:51:51.411828 131401512719488 run.py:519] Algo bellman_ford step 373 current loss 0.150706, current_train_items 11968.\n",
      "I0215 11:51:51.939563 131401512719488 run.py:519] Algo bellman_ford step 374 current loss 0.113247, current_train_items 12000.\n",
      "I0215 11:51:51.966261 131401512719488 run.py:519] Algo bellman_ford step 375 current loss 0.007946, current_train_items 12032.\n",
      "I0215 11:51:52.055083 131401512719488 run.py:519] Algo bellman_ford step 376 current loss 0.031384, current_train_items 12064.\n",
      "I0215 11:51:52.258449 131401512719488 run.py:519] Algo bellman_ford step 377 current loss 0.081422, current_train_items 12096.\n",
      "I0215 11:51:52.610743 131401512719488 run.py:519] Algo bellman_ford step 378 current loss 0.211679, current_train_items 12128.\n",
      "I0215 11:51:53.140893 131401512719488 run.py:519] Algo bellman_ford step 379 current loss 0.121782, current_train_items 12160.\n",
      "I0215 11:51:53.166610 131401512719488 run.py:519] Algo bellman_ford step 380 current loss 0.005002, current_train_items 12192.\n",
      "I0215 11:51:53.257242 131401512719488 run.py:519] Algo bellman_ford step 381 current loss 0.082958, current_train_items 12224.\n",
      "I0215 11:51:53.457376 131401512719488 run.py:519] Algo bellman_ford step 382 current loss 0.194911, current_train_items 12256.\n",
      "I0215 11:51:53.806261 131401512719488 run.py:519] Algo bellman_ford step 383 current loss 0.164352, current_train_items 12288.\n",
      "I0215 11:51:54.336245 131401512719488 run.py:519] Algo bellman_ford step 384 current loss 0.318813, current_train_items 12320.\n",
      "I0215 11:51:54.361974 131401512719488 run.py:519] Algo bellman_ford step 385 current loss 0.062249, current_train_items 12352.\n",
      "I0215 11:51:54.446531 131401512719488 run.py:519] Algo bellman_ford step 386 current loss 0.132133, current_train_items 12384.\n",
      "I0215 11:51:54.648482 131401512719488 run.py:519] Algo bellman_ford step 387 current loss 0.264137, current_train_items 12416.\n",
      "I0215 11:51:54.988123 131401512719488 run.py:519] Algo bellman_ford step 388 current loss 0.233169, current_train_items 12448.\n",
      "I0215 11:51:55.513244 131401512719488 run.py:519] Algo bellman_ford step 389 current loss 0.318222, current_train_items 12480.\n",
      "I0215 11:51:55.538406 131401512719488 run.py:519] Algo bellman_ford step 390 current loss 0.039812, current_train_items 12512.\n",
      "I0215 11:51:55.627982 131401512719488 run.py:519] Algo bellman_ford step 391 current loss 0.098943, current_train_items 12544.\n",
      "I0215 11:51:55.826495 131401512719488 run.py:519] Algo bellman_ford step 392 current loss 0.150160, current_train_items 12576.\n",
      "I0215 11:51:56.175393 131401512719488 run.py:519] Algo bellman_ford step 393 current loss 0.162604, current_train_items 12608.\n",
      "I0215 11:51:56.710524 131401512719488 run.py:519] Algo bellman_ford step 394 current loss 0.159416, current_train_items 12640.\n",
      "I0215 11:51:56.736458 131401512719488 run.py:519] Algo bellman_ford step 395 current loss 0.014538, current_train_items 12672.\n",
      "I0215 11:51:56.827162 131401512719488 run.py:519] Algo bellman_ford step 396 current loss 0.017933, current_train_items 12704.\n",
      "I0215 11:51:57.030243 131401512719488 run.py:519] Algo bellman_ford step 397 current loss 0.197673, current_train_items 12736.\n",
      "I0215 11:51:57.369040 131401512719488 run.py:519] Algo bellman_ford step 398 current loss 0.060478, current_train_items 12768.\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m clrs.examples.run --algorithms \"bellman_ford\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89fe8754-500a-46e0-972f-862592f284b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 20:38:34.053036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740343114.064627   10028 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740343114.067839   10028 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import os\n",
    "import shutil\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import clrs\n",
    "import jax\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def visualize_graph_from_adjacency_matrix(adjacency_matrix, weight_matrix=None):\n",
    "    \"\"\"\n",
    "    Visualizes a graph with explicit arrows and labeled edge weights (adjacent).\n",
    "\n",
    "    Args:\n",
    "        adjacency_matrix: Adjacency matrix (NumPy array).\n",
    "        weight_matrix: Optional weight matrix (NumPy array).\n",
    "    \"\"\"\n",
    "\n",
    "    adjacency_matrix = np.array(adjacency_matrix)\n",
    "    if adjacency_matrix.shape[0] != adjacency_matrix.shape[1]:\n",
    "        raise ValueError(\"Adjacency matrix must be square.\")\n",
    "    num_nodes = adjacency_matrix.shape[0]\n",
    "\n",
    "    if weight_matrix is None:\n",
    "        weight_matrix = np.ones_like(adjacency_matrix)\n",
    "    else:\n",
    "        weight_matrix = np.array(weight_matrix)\n",
    "        if weight_matrix.shape != adjacency_matrix.shape:\n",
    "            raise ValueError(\"Weight matrix must have the same dimensions.\")\n",
    "\n",
    "    directed_graph = nx.DiGraph()\n",
    "    undirected_graph = nx.Graph()\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        directed_graph.add_node(i)\n",
    "        undirected_graph.add_node(i)\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if adjacency_matrix[i, j] != 0:\n",
    "                    weight = round(weight_matrix[i, j], 2)\n",
    "                    if adjacency_matrix[j, i] != 0:\n",
    "                        if i < j:\n",
    "                            undirected_graph.add_edge(i, j, weight=weight)\n",
    "                    else:\n",
    "                        directed_graph.add_edge(i, j, weight=weight)\n",
    "\n",
    "    pos = nx.spring_layout(undirected_graph)  # Layout based on undirected\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Draw undirected edges (no arrows)\n",
    "    nx.draw_networkx_edges(undirected_graph, pos, edge_color='gray', width=2, arrows=False)\n",
    "    edge_labels_undirected = nx.get_edge_attributes(undirected_graph, 'weight')\n",
    "    # Use label_pos and rotate for adjacent labels\n",
    "    nx.draw_networkx_edge_labels(undirected_graph, pos, edge_labels=edge_labels_undirected,\n",
    "                                 label_pos=0.3, rotate=True)\n",
    "\n",
    "    # Draw directed edges with explicit arrows\n",
    "    nx.draw_networkx_edges(directed_graph, pos, edge_color='black', width=1,\n",
    "                           arrowstyle='->', arrowsize=15)\n",
    "    edge_labels_directed = nx.get_edge_attributes(directed_graph, 'weight')\n",
    "    # Use label_pos and rotate for adjacent labels\n",
    "    nx.draw_networkx_edge_labels(directed_graph, pos, edge_labels=edge_labels_directed,\n",
    "                                 label_pos=0.3, rotate=True)\n",
    "\n",
    "    nx.draw_networkx_nodes(directed_graph, pos, node_color='skyblue', node_size=500)\n",
    "    nx.draw_networkx_labels(directed_graph, pos)\n",
    "\n",
    "    plt.title(\"Graph Visualization\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "encode_hints = True\n",
    "decode_hints = True\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2**32))\n",
    "\n",
    "# Create samplers\n",
    "sampler, spec = clrs.build_sampler(\n",
    "        \"bellman_ford\",\n",
    "        seed=rng.randint(2**32),\n",
    "        num_samples=NUM_SAMPLES,\n",
    "        length=7,\n",
    "        )\n",
    "\n",
    "processor_factory = clrs.get_processor_factory(\n",
    "    'triplet_gmpnn',\n",
    "    use_ln=True,\n",
    "    nb_triplet_fts=8,\n",
    "    nb_heads=1,\n",
    "    )\n",
    "model_params = dict(\n",
    "    processor_factory=processor_factory,\n",
    "    hidden_dim=128,\n",
    "    encode_hints=encode_hints,\n",
    "    decode_hints=decode_hints,\n",
    "    encoder_init='xavier_on_scalars',\n",
    "    use_lstm=False,\n",
    "    learning_rate=0.001,\n",
    "    grad_clip_max_norm=1.0,\n",
    "    checkpoint_path='checkpoints/CLRS30',\n",
    "    freeze_processor=False,\n",
    "    dropout_prob=0.0,\n",
    "    hint_teacher_forcing=0.0,\n",
    "    hint_repred_mode='soft',\n",
    "    nb_msg_passing_steps=1,\n",
    "    )\n",
    "dummy_traj = [sampler.next()]\n",
    "model = clrs.models.BaselineModel(\n",
    "  spec=[spec],\n",
    "  dummy_trajectory=dummy_traj,\n",
    "    get_inter=True,\n",
    "  **model_params\n",
    ")\n",
    "\n",
    "# feedback_list = [next(t) for t in train_samplers]\n",
    "\n",
    "# # Initialize model.\n",
    "all_features = [f.features for f in dummy_traj]\n",
    "model.init(all_features, 43)\n",
    "\n",
    "model.restore_model('best.pkl', only_load_processor=False)\n",
    "\n",
    "\n",
    "feedback = sampler.next()\n",
    "# batch_size = feedback.outputs[0].data.shape[0]\n",
    "new_rng_key, rng_key = jax.random.split(rng_key)\n",
    "\n",
    "preds, _, hist = model.predict(new_rng_key, feedback.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bac77685-6144-491f-b1bc-9bb1a7ccaa4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback.outputs[0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced6a6ef-8322-47ce-9444-d77e34f7768f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "item = random.randint(0, NUM_SAMPLES-1)\n",
    "weights = feedback.features.inputs[2].data[item]\n",
    "adj = feedback.features.inputs[3].data[item]\n",
    "visualize_graph_from_adjacency_matrix(adj, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ba9cac-33f0-4733-b1d0-96cebaa687d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([6., 3., 2., 5., 6., 6., 6.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds['pi'].data[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838435ff-bcd8-4795-a186-fe1383ae9484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 3, 2, 5, 6, 6, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback.outputs[0].data[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23cf118-71ef-4e20-81ec-961a76c16d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_distances = feedback.features.hints[1].data[:,item,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f688d3a6-b4f2-431a-a6a4-54988b4a02f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist[0].hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb8d16a1-f2a0-4dd3-b0ba-181636d8a963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dad52c2c-665c-4c3d-93c1-9d124ef6fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = np.stack([hist[i].hiddens[item] for i in range(7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccdd01a2-b935-47a7-bca5-cca7352c013f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a48cbce1-3561-4f4a-93e5-659bb0cadd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.57060470e-04, -4.42666678e-05, -1.04704932e-05,\n",
       "        -3.75850250e-05,  4.79483128e-01, -2.50252366e-01,\n",
       "        -3.70225207e-05,  5.41632287e-02, -6.05110581e-05,\n",
       "        -3.17516953e-01, -5.64759597e-04, -3.03903071e-05,\n",
       "        -2.89645941e-05,  3.39706707e+00, -7.52401352e-02,\n",
       "        -2.08628444e-05,  1.93750274e+00,  5.02030075e-01,\n",
       "         9.98109877e-02, -6.11152276e-02, -6.14503224e-05,\n",
       "        -4.17179195e-04, -3.72906357e-01, -1.08084013e-03,\n",
       "        -2.62076946e-06, -3.57461363e-01, -1.90890342e-01,\n",
       "        -8.39518634e-06, -1.18387426e-04, -8.04630236e-06,\n",
       "        -6.06710455e-05, -1.09281227e-05, -7.65131265e-02,\n",
       "        -1.37300915e-04, -5.26535571e-01, -1.11410409e-01,\n",
       "        -2.40803659e-01,  2.59074237e-04, -2.37633184e-01,\n",
       "        -1.19179051e-04, -8.25755415e-04, -9.24350787e-03,\n",
       "         2.06790471e+00, -1.84835858e-06, -5.93922473e-03,\n",
       "        -1.76624280e-06,  1.60817027e+00, -3.06238770e-01,\n",
       "         3.63948417e+00, -5.16292603e-05,  1.18955934e+00,\n",
       "        -1.63608924e-01, -5.28394520e-01, -1.33121270e-04,\n",
       "        -1.04299046e-01,  1.82422912e+00, -3.39810981e-06,\n",
       "        -6.38913116e-05, -3.81410719e-05,  1.27711165e+00,\n",
       "        -9.36073920e-05, -4.21887817e-05, -1.02986123e-04,\n",
       "        -5.83672008e-05, -1.74543142e-01, -1.83514738e-03,\n",
       "        -5.43693113e-06, -3.17085296e-01,  6.09948099e-01,\n",
       "        -6.70977216e-03, -1.58725749e-03, -2.02824503e-01,\n",
       "        -9.75490138e-07, -2.54419683e-05, -4.48628543e-06,\n",
       "        -1.78727835e-01, -1.71549261e-01, -4.48191583e-01,\n",
       "        -2.40479014e-03, -3.09931606e-01, -8.36327163e-05,\n",
       "        -4.88487567e-05,  4.14755440e+00, -1.49614304e-01,\n",
       "        -1.49000615e-01, -4.39397871e-01, -1.46386549e-01,\n",
       "        -1.65705569e-03, -8.93438992e-05, -2.42832939e-05,\n",
       "        -1.03796374e-05, -6.09848974e-03, -2.02391721e-08,\n",
       "        -3.75294667e-06, -6.87306689e-04, -4.75504989e-04,\n",
       "        -1.06583888e-04,  4.10716057e+00, -1.40496274e-03,\n",
       "        -2.39710882e-01, -1.04060084e-01, -2.84601760e-04,\n",
       "        -2.23548174e-01, -2.48690754e-01, -1.70749590e-05,\n",
       "        -4.60440097e-05,  2.17732549e+00, -1.55923190e-04,\n",
       "        -2.54973915e-04, -2.69586861e-01, -2.04922744e-05,\n",
       "        -2.36322097e-02, -1.92530931e-03, -7.44691351e-05,\n",
       "        -2.17298418e-01, -6.53070456e-05,  8.37469518e-01,\n",
       "        -2.11043790e-01, -6.01640349e-05,  2.92436218e+00,\n",
       "         7.09934397e-07,  1.38102782e+00, -1.16379706e-05,\n",
       "        -1.06959858e-04,  4.43893336e-02, -3.73378575e-01,\n",
       "        -1.06883104e-04, -5.63041576e-05],\n",
       "       [-9.15259705e-04, -1.88610022e-04, -7.37661242e-01,\n",
       "        -5.41169172e-08, -7.33245432e-01,  3.16640168e-01,\n",
       "        -3.31684191e-06, -6.73105061e-01, -5.16262588e-09,\n",
       "         2.21648793e-06,  9.07975074e-04, -8.89526129e-01,\n",
       "        -8.70341480e-01, -2.30317437e-05, -6.03181541e-01,\n",
       "        -1.36759295e-03, -5.74811578e-01, -5.11331797e-01,\n",
       "        -6.08586371e-01, -1.98049776e-07,  5.60680746e-08,\n",
       "        -6.21107340e-01,  4.09937763e+00, -4.06063947e-08,\n",
       "        -2.14924850e-03,  7.45991611e+00, -2.64114642e-05,\n",
       "        -7.72458255e-01, -6.67772115e-10, -7.79766798e-01,\n",
       "        -3.87658834e-01, -4.11291876e-06,  1.09920315e-04,\n",
       "        -2.25885202e-07,  2.68255401e+00,  1.68925357e+00,\n",
       "         1.49207664e+00, -6.32802308e-01, -2.94375610e-07,\n",
       "         1.00208389e-07, -6.70379996e-01, -1.11764006e-01,\n",
       "        -7.31700778e-01, -2.21621388e-10, -2.23399431e-01,\n",
       "        -2.02115938e-01, -3.82446558e-07, -3.48235607e-01,\n",
       "        -2.94761253e-06, -1.46559557e-06, -5.86970031e-01,\n",
       "         2.25924705e-05, -8.67622418e-08, -8.31613183e-01,\n",
       "         4.24788741e-05,  4.09879446e-01, -7.41615474e-01,\n",
       "        -8.62721645e-04, -8.58963613e-05, -1.32863192e-04,\n",
       "        -6.30451552e-07, -4.10039174e-05, -5.10908023e-07,\n",
       "        -5.23811638e-01, -2.18805116e-08,  5.05560216e-08,\n",
       "         2.94007805e-05, -1.93516428e-07, -8.80253494e-01,\n",
       "        -5.82953191e-07, -7.24875872e-06,  1.71220338e-03,\n",
       "        -1.73721986e-07, -2.21558381e-08,  7.12965381e-11,\n",
       "         8.68709944e-03, -2.50011283e-07,  7.46859312e-01,\n",
       "         9.84682739e-01, -4.42468941e-01,  6.98485735e-07,\n",
       "        -5.99972017e-11, -5.71417093e-01,  2.16193584e-05,\n",
       "         6.71148612e-08,  1.72625172e+00,  5.43278396e-01,\n",
       "        -6.90218747e-01, -1.29195541e-07, -6.85362637e-01,\n",
       "        -1.62388867e-04,  2.52568884e-06, -1.83588526e-07,\n",
       "        -1.11966733e-06, -8.24796438e-01,  3.65804738e-07,\n",
       "        -4.10906087e-10, -2.89100603e-06, -1.28013460e-06,\n",
       "        -3.59754086e-01, -2.66795848e-08,  6.67574625e-07,\n",
       "         3.14466810e+00, -1.47114214e-08,  1.02212061e-06,\n",
       "        -1.30284604e-06,  1.97196925e+00, -1.44607540e-07,\n",
       "        -3.88958279e-06, -4.10457671e-01, -7.49939915e-08,\n",
       "         6.56260672e-05, -3.36394005e-04, -8.38555115e-06,\n",
       "        -2.58895993e-01, -6.83182210e-04, -7.52976775e-01,\n",
       "         8.46863557e-08, -7.11151515e-05, -7.46544838e-01,\n",
       "        -6.39591576e-07, -5.98429084e-01, -8.15618455e-01,\n",
       "        -2.05303479e-07, -4.40146744e-01, -5.23352563e-01,\n",
       "        -6.79931247e-07, -7.41367996e-01],\n",
       "       [-4.14279988e-04, -3.07367824e-04, -7.38527656e-01,\n",
       "        -1.49880194e-07, -7.34062254e-01,  5.96575439e-01,\n",
       "        -4.37417066e-06, -6.73975885e-01, -2.21111360e-08,\n",
       "         4.49044410e-06,  5.82948793e-04, -8.90561700e-01,\n",
       "        -8.74084771e-01, -6.89279768e-05, -6.07439101e-01,\n",
       "        -1.27214543e-03, -5.75551987e-01, -5.11877775e-01,\n",
       "        -6.09242916e-01, -5.95893880e-07,  9.90328672e-08,\n",
       "        -6.21869802e-01,  3.94033051e+00, -7.41993489e-08,\n",
       "        -4.06894879e-03,  7.35628223e+00, -2.05289325e-05,\n",
       "        -7.77996421e-01, -1.76150494e-09, -7.87347615e-01,\n",
       "        -3.88321668e-01, -2.23337474e-06,  1.54758847e-04,\n",
       "        -2.88021624e-07,  2.56154323e+00,  1.61458921e+00,\n",
       "         1.30816889e+00, -6.33532345e-01, -3.74704769e-07,\n",
       "         6.13020701e-09, -6.71156526e-01, -1.12309538e-01,\n",
       "        -7.32475221e-01, -1.24377320e-09, -2.28906333e-01,\n",
       "        -2.60572106e-01,  5.76413441e-08, -3.28726470e-01,\n",
       "        -4.13446469e-06, -2.25072199e-06, -5.87755978e-01,\n",
       "         1.39705853e-05, -1.05364131e-06, -8.32496107e-01,\n",
       "         2.50892626e-05,  2.84831613e-01, -7.52327561e-01,\n",
       "        -5.85699105e-04, -4.23352358e-05, -1.10328030e-04,\n",
       "        -1.49726134e-06, -3.35648874e-05, -1.91168942e-06,\n",
       "        -5.24298310e-01, -6.26908871e-08,  8.81900135e-08,\n",
       "        -2.19965950e-05, -7.21183426e-07, -8.81203055e-01,\n",
       "        -1.51700431e-06, -1.85430872e-05,  5.06193750e-03,\n",
       "        -3.23557629e-07, -5.24443315e-08, -1.24913935e-09,\n",
       "         2.92942743e-03, -7.50132017e-07,  8.66770446e-01,\n",
       "         1.09438396e+00, -4.43252057e-01,  3.84456143e-06,\n",
       "        -2.43173509e-10, -5.72087586e-01,  1.37203397e-05,\n",
       "         8.40289047e-08,  1.75654554e+00,  4.37505245e-01,\n",
       "        -6.90827966e-01, -7.24159520e-07, -6.86185658e-01,\n",
       "        -1.07184605e-04,  2.46849822e-06, -7.04620930e-08,\n",
       "        -4.99343832e-06, -8.25973749e-01,  7.23791516e-07,\n",
       "        -1.02947273e-09,  1.76690883e-05, -6.44091369e-06,\n",
       "        -3.60546649e-01, -2.44854562e-07,  7.93511617e-07,\n",
       "         3.15275860e+00, -2.59757886e-08,  1.56819181e-06,\n",
       "        -1.40014004e-06,  2.01215696e+00, -2.74391567e-07,\n",
       "        -7.78481808e-06, -4.11206573e-01, -2.16950951e-07,\n",
       "         1.44507052e-04, -1.02997619e-04, -4.76900277e-06,\n",
       "        -2.39502698e-01, -3.90187255e-04, -7.53679216e-01,\n",
       "         2.07188691e-07, -6.43352687e-05, -7.47314811e-01,\n",
       "        -5.20829587e-07, -5.88557005e-01, -8.16516042e-01,\n",
       "        -6.57172279e-07, -4.40851539e-01, -5.24169862e-01,\n",
       "        -1.58991668e-06, -7.42195129e-01],\n",
       "       [-3.88633838e-04, -4.09672328e-04, -6.06151521e-01,\n",
       "        -1.47786992e-07,  6.41001344e-01, -2.36536607e-01,\n",
       "        -1.05929448e-05,  2.70678103e-01, -2.18550991e-08,\n",
       "        -3.47792596e-01,  1.03289972e-03, -7.62803465e-05,\n",
       "        -6.72057911e-04,  4.24886751e+00, -5.94085315e-04,\n",
       "        -4.11663670e-04,  1.63845527e+00,  2.14685321e-01,\n",
       "        -5.03009915e-01, -1.86736643e-01,  7.87743559e-08,\n",
       "        -5.05432010e-01, -3.56805235e-01, -4.30494361e-03,\n",
       "        -6.53846515e-03, -3.41529429e-01, -1.87311918e-01,\n",
       "        -5.03954398e-05, -9.84570576e-08, -6.72233364e-05,\n",
       "        -2.87078679e-01, -6.09940741e-07, -2.30834261e-01,\n",
       "        -4.24610931e-07, -5.07240176e-01, -2.83625484e-01,\n",
       "        -2.27744609e-01, -5.22020161e-01, -2.44353831e-01,\n",
       "         3.78609393e-08, -8.39310116e-04, -1.80819537e-02,\n",
       "         2.54141355e+00, -2.68766964e-09, -2.60705259e-02,\n",
       "        -7.69105647e-03,  1.30173600e+00, -3.16912830e-01,\n",
       "         3.38146996e+00, -5.60067610e-06,  9.81845915e-01,\n",
       "        -1.50155768e-01, -1.46296338e-06, -6.97635174e-01,\n",
       "        -1.12420671e-01,  1.68263018e+00, -2.97785475e-04,\n",
       "        -5.70432167e-04, -3.74944175e-05,  1.92463517e+00,\n",
       "        -3.56413534e-06, -5.66560993e-05, -2.24650398e-06,\n",
       "        -4.13262874e-01, -1.84781507e-01, -7.33355235e-04,\n",
       "        -7.83951964e-06, -3.04455727e-01,  6.15762055e-01,\n",
       "        -3.94626795e-06, -8.69879659e-06,  4.35281033e-03,\n",
       "        -8.13531472e-07, -9.35138900e-08, -4.95368058e-09,\n",
       "        -1.65959463e-01, -1.78017989e-01,  1.98992625e-01,\n",
       "         1.77456103e-02, -3.08016717e-01,  9.59180852e-06,\n",
       "        -5.82124460e-10,  4.34326077e+00, -1.38663858e-01,\n",
       "        -1.57947078e-01, -4.22561735e-01, -1.35104969e-01,\n",
       "        -5.44945121e-01, -5.59677915e-07, -5.60466409e-01,\n",
       "        -4.29927459e-05,  6.41430370e-06, -2.25510668e-08,\n",
       "        -8.31637681e-06, -3.17795272e-03,  1.52755103e-06,\n",
       "        -1.99431804e-09,  3.47996497e+00, -8.56962288e-06,\n",
       "        -2.24286065e-01, -3.84318759e-03,  8.09662595e-07,\n",
       "        -2.28049546e-01, -2.36746565e-01,  2.41617772e-06,\n",
       "        -3.21282687e-06,  1.45626712e+00, -4.53352840e-07,\n",
       "        -1.81574014e-05, -2.82346547e-01, -4.06927455e-07,\n",
       "        -1.45163925e-04, -3.72900307e-04, -6.25920302e-06,\n",
       "        -2.25372121e-01, -8.51186691e-04,  1.67342448e+00,\n",
       "        -1.99137360e-01, -4.09604145e-05,  3.09022665e+00,\n",
       "        -7.70875261e-07,  1.16262507e+00, -6.79419994e-01,\n",
       "        -6.31164994e-07, -3.19106817e-01, -1.03290625e-01,\n",
       "        -1.80342602e-06, -6.15676939e-01],\n",
       "       [-3.08007468e-04, -6.30100476e-05, -8.61508215e-06,\n",
       "        -1.91526837e-04,  1.22779942e+00, -2.55714029e-01,\n",
       "        -8.34992752e-05,  2.04680586e+00, -2.97703427e-05,\n",
       "        -3.51757467e-01, -4.64545470e-03, -2.96559429e-05,\n",
       "        -4.24869249e-05,  4.45535755e+00, -7.86082912e-03,\n",
       "        -1.26857878e-04,  1.03300476e+00,  2.75264904e-02,\n",
       "         6.74580336e-01, -2.27542773e-01, -1.48844934e-04,\n",
       "        -7.91611528e-05, -3.77801001e-01, -1.78745732e-01,\n",
       "        -2.78943771e-05, -3.63902092e-01, -1.95344329e-01,\n",
       "        -2.43020258e-05, -2.81369521e-05,  1.93288033e-05,\n",
       "        -5.23546332e-05, -1.01136075e-05, -3.85241359e-01,\n",
       "        -1.76009402e-04, -5.34301341e-01, -3.41641188e-01,\n",
       "        -2.46117592e-01,  4.64045629e-03, -2.47916877e-01,\n",
       "        -1.24472848e-04, -5.61547279e-03, -2.47158334e-02,\n",
       "         2.91281056e+00, -1.23666030e-06, -1.42433553e-03,\n",
       "        -1.27801950e-05,  3.38126510e-01, -3.20976347e-01,\n",
       "         1.95817971e+00, -2.06591449e-05,  3.25775772e-01,\n",
       "        -1.68634087e-01, -5.36196172e-01, -3.47778296e-05,\n",
       "        -1.15648046e-01,  9.64127183e-01, -1.20843742e-05,\n",
       "        -3.18955339e-04, -5.38735221e-05,  2.04430246e+00,\n",
       "        -1.81408977e-04, -2.21018108e-05, -2.53515085e-04,\n",
       "        -5.32146732e-05, -1.88255310e-01, -1.66527793e-01,\n",
       "        -9.67463493e-05, -3.21307927e-01,  2.79153919e+00,\n",
       "        -1.19665623e-01, -1.52115207e-02, -2.45686054e-01,\n",
       "        -7.44637589e-07, -5.65923583e-05, -7.81933850e-05,\n",
       "        -1.84274018e-01, -1.81594506e-01, -4.54382986e-01,\n",
       "        -1.39599759e-03,  3.35051417e-01, -2.80775275e-04,\n",
       "        -1.02522667e-04,  4.26883554e+00, -1.54064089e-01,\n",
       "        -1.61241606e-01, -4.41023797e-01, -1.51484400e-01,\n",
       "        -5.00122027e-04, -5.56596380e-04, -1.51351478e-05,\n",
       "        -9.88454412e-05, -1.36133954e-01, -1.75828963e-09,\n",
       "        -8.96805341e-06, -8.75167825e-05, -5.94972458e-04,\n",
       "        -2.23197363e-04,  2.29839587e+00, -3.18215452e-02,\n",
       "        -2.45952815e-01, -1.58073410e-01, -3.11682303e-03,\n",
       "        -2.33082250e-01, -2.53255069e-01, -9.92198984e-05,\n",
       "        -2.43906998e-05,  9.69975531e-01, -7.71246478e-03,\n",
       "        -1.31550143e-04,  1.96628809e-01, -2.79413245e-04,\n",
       "        -1.63317816e-06, -2.15607724e-05, -1.36777307e-04,\n",
       "        -2.28952453e-01, -2.48151737e-05,  1.66832373e-01,\n",
       "        -2.15837076e-01, -2.39508969e-04,  4.22747755e+00,\n",
       "         3.38886166e-05,  1.87247023e-01, -9.72348644e-05,\n",
       "        -2.05184985e-02,  6.61103964e-01,  1.12606740e+00,\n",
       "        -2.09498430e-05, -2.73472571e-04],\n",
       "       [-2.02915515e-04, -4.20491851e-05, -1.71849138e-06,\n",
       "        -1.96888912e-04,  1.61521065e+00, -2.52365738e-01,\n",
       "        -4.63445576e-05,  2.93514538e+00, -5.12071392e-06,\n",
       "        -3.51205677e-01, -1.83373783e-02, -7.42664997e-06,\n",
       "        -1.92478055e-05,  4.65609026e+00, -1.11205655e-03,\n",
       "        -8.10552592e-05,  3.01465303e-01, -1.19060889e-01,\n",
       "         9.20885921e-01, -2.28204608e-01, -1.93690168e-04,\n",
       "        -2.89041218e-05, -3.75416279e-01, -1.86357170e-01,\n",
       "        -1.18690068e-05, -3.59958351e-01, -1.95175812e-01,\n",
       "        -9.63292405e-06, -1.70454259e-05,  5.19600599e-06,\n",
       "        -4.04709062e-05, -5.53339805e-06, -3.85235965e-01,\n",
       "        -2.26130505e-05, -5.29631138e-01, -3.43432039e-01,\n",
       "        -2.43048653e-01,  3.58163891e-03, -2.47411102e-01,\n",
       "        -9.54308562e-05,  1.31045207e-02, -2.90713664e-02,\n",
       "         3.19868612e+00, -5.90308275e-07, -9.06124013e-04,\n",
       "        -8.23546725e-06, -4.71320927e-01, -3.20889354e-01,\n",
       "         8.16041946e-01, -1.16731444e-05, -2.58309543e-01,\n",
       "        -1.65859744e-01, -5.32546759e-01, -3.18283492e-05,\n",
       "        -1.15182735e-01,  4.03490394e-01, -2.47320418e-06,\n",
       "        -1.46964245e-04, -3.71796705e-05,  2.31823087e+00,\n",
       "        -1.24392755e-04, -4.57661190e-05, -1.81764204e-04,\n",
       "        -3.98351949e-05, -1.87779129e-01, -1.85759366e-01,\n",
       "        -4.67603131e-05, -3.19003820e-01,  3.64368701e+00,\n",
       "        -2.85904646e-01, -2.08201036e-02, -2.45309055e-01,\n",
       "        -2.28636125e-07, -4.12377849e-05, -7.40363976e-05,\n",
       "        -1.81074888e-01, -1.81149617e-01, -4.50613558e-01,\n",
       "        -1.38758289e-04,  1.23464227e+00, -4.89815429e-05,\n",
       "        -1.01829275e-04,  3.94396496e+00, -1.51346892e-01,\n",
       "        -1.60773829e-01, -4.34642017e-01, -1.48683190e-01,\n",
       "        -1.53948771e-04, -3.90644127e-04, -2.19467893e-06,\n",
       "        -1.00304103e-04, -1.68366417e-01, -4.61915728e-10,\n",
       "        -2.56714247e-06, -1.81453124e-05, -5.04049414e-04,\n",
       "        -1.88615799e-04,  9.20055747e-01, -1.30510956e-01,\n",
       "        -2.41720468e-01, -1.58923566e-01, -1.08293789e-02,\n",
       "        -2.32434481e-01, -2.50019640e-01, -3.34514334e-05,\n",
       "        -1.77461534e-05,  2.28096649e-01, -6.39554039e-02,\n",
       "        -7.01838871e-05,  1.36450517e+00, -2.22193004e-04,\n",
       "        -8.01304964e-07, -3.59896853e-06, -2.50314130e-04,\n",
       "        -2.28556693e-01, -1.97611917e-05, -8.31412673e-02,\n",
       "        -2.12847412e-01, -2.37801680e-04,  4.63363504e+00,\n",
       "         3.64735351e-05, -4.70328003e-01, -4.41545017e-05,\n",
       "        -6.75913915e-02,  8.61944973e-01,  2.20256066e+00,\n",
       "        -5.33009370e-06, -1.45544371e-04],\n",
       "       [-3.02221025e-07, -1.84779914e-09, -8.11501338e-14,\n",
       "        -2.31852315e-11,  1.59122741e+00, -2.43091568e-01,\n",
       "        -1.00377598e-07,  2.43685889e+00, -1.24487677e-11,\n",
       "        -3.52725357e-01, -6.41735387e-05, -8.20426216e-09,\n",
       "        -4.54050599e-12,  5.06094265e+00, -1.14960130e-04,\n",
       "        -2.19641083e-09,  8.31177175e-01,  3.24061632e-01,\n",
       "         4.56957966e-01, -2.28863627e-01, -4.84197074e-07,\n",
       "        -4.81478768e-09, -3.64705890e-01, -1.86340898e-01,\n",
       "        -7.02620323e-11, -3.49132478e-01, -1.93066344e-01,\n",
       "        -1.33100345e-11, -3.25538281e-08,  2.53284997e-11,\n",
       "        -1.65102954e-08, -1.78065462e-09, -3.85934711e-01,\n",
       "        -2.16400994e-10, -5.16491950e-01, -3.33764553e-01,\n",
       "        -2.34098911e-01,  8.71895224e-07, -2.50291944e-01,\n",
       "        -9.88373330e-08, -4.80290852e-04, -2.31989641e-02,\n",
       "         3.30338073e+00, -1.74834851e-11, -1.32451387e-04,\n",
       "        -6.36843573e-12, -3.10559332e-01, -3.22888702e-01,\n",
       "         1.27777934e+00, -2.75238659e-08,  1.69134811e-01,\n",
       "        -1.56836271e-01, -5.22674799e-01, -8.23640212e-10,\n",
       "        -1.17529131e-01,  6.09074235e-01, -2.86551841e-11,\n",
       "        -1.33705136e-08, -1.52058621e-09,  2.31884336e+00,\n",
       "        -1.47447263e-10, -1.36905021e-08, -3.67029193e-12,\n",
       "        -5.80687887e-10, -1.90394655e-01, -1.48375213e-01,\n",
       "        -1.54218804e-09, -3.10659081e-01,  2.97248507e+00,\n",
       "        -2.39419416e-01, -3.80550813e-10, -2.37425029e-01,\n",
       "        -1.11992214e-14, -2.45744813e-10, -6.90158970e-08,\n",
       "        -1.72196373e-01, -1.83760464e-01, -4.40305412e-01,\n",
       "        -1.86243847e-06,  7.91160703e-01, -4.18478919e-07,\n",
       "        -8.05997213e-10,  4.15398502e+00, -1.43893853e-01,\n",
       "        -1.63366497e-01, -4.30769205e-01, -1.40708849e-01,\n",
       "        -7.18573647e-05, -4.00443810e-11, -1.58549611e-07,\n",
       "        -1.44797663e-11, -3.97228517e-09, -1.87519621e-16,\n",
       "        -8.82514808e-12, -2.53261495e-10, -1.39608233e-07,\n",
       "         3.35901174e-09,  1.51126742e+00, -2.41389056e-03,\n",
       "        -2.31689095e-01, -1.51526392e-01, -4.46902186e-06,\n",
       "        -2.34867454e-01, -2.42463574e-01, -1.13325148e-10,\n",
       "        -2.25316228e-08,  1.04076182e-02, -5.93903205e-05,\n",
       "        -3.59995145e-09,  7.29324818e-01, -1.95014014e-08,\n",
       "        -1.54542368e-09, -3.00544514e-07, -5.23365861e-06,\n",
       "        -2.31272891e-01, -5.27479171e-09,  8.33815575e-01,\n",
       "        -2.04819158e-01, -8.71022543e-09,  4.46351385e+00,\n",
       "         1.17707248e-08, -1.02068044e-01, -1.78768242e-12,\n",
       "        -3.88323169e-14, -1.63048878e-01,  1.56902874e+00,\n",
       "        -1.71465819e-09, -4.15438323e-10]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c9ebdb6-8c0a-4af8-8563-6c0d87dedd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_distances = np.stack([hist[i].hint_preds['d'][item] for i in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5862a4c5-89b2-413d-8ae7-1dad4ed34466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0181314 ,  0.02040932, -0.04741773,  0.02251913,  0.3484529 ,\n",
       "         0.02420866,  0.02549018],\n",
       "       [-0.01688743,  0.7240098 , -0.04077971,  0.64844507,  0.28606293,\n",
       "        -0.01042079, -0.01252194],\n",
       "       [-0.02653136,  0.7457953 , -0.05056605,  0.6698984 ,  0.3377769 ,\n",
       "         0.9282364 , -0.00698071],\n",
       "       [-0.0286141 ,  0.7400521 , -0.03928775,  0.661202  ,  0.29927325,\n",
       "         0.9333077 ,  1.1851364 ],\n",
       "       [ 1.904062  ,  0.7563996 , -0.04483517,  0.6747404 ,  0.32863095,\n",
       "         0.92498463,  1.1660279 ],\n",
       "       [ 1.8337085 ,  0.7490817 , -0.04533114,  0.6705845 ,  0.3092793 ,\n",
       "         0.9339648 ,  1.1737171 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c303821-f01d-4b64-b330-91adee063f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.34155027,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.74841518, 0.        , 0.67207262, 0.34155027,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.74841518, 0.        , 0.67207262, 0.34155027,\n",
       "        0.94396969, 0.        ],\n",
       "       [0.        , 0.74841518, 0.        , 0.67207262, 0.34155027,\n",
       "        0.94396969, 1.163387  ],\n",
       "       [1.84615435, 0.74841518, 0.        , 0.67207262, 0.34155027,\n",
       "        0.94396969, 1.163387  ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98d64058-4638-4ba5-b143-6083d13fc77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataPoint(name=\"pi\",\tlocation=node,\ttype=pointer,\tdata=Array(1000, 7))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback.outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385de87-68a6-407c-8c8d-bbc4a1f17ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
