{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d714ad",
   "metadata": {},
   "source": [
    "# Joint Network Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c937ca6e-48bd-40f4-b6fb-4f8efe119b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'data/bellman_ford_bfs/interp_data_OOD_eval.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m alg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbellman_ford_bfs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m data_source \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, alg)\n\u001b[0;32m---> 10\u001b[0m val_dataset_ood \u001b[38;5;241m=\u001b[39m HDF5Dataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, alg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterp_data_OOD_eval.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m), nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m val_dataloader_ood \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset_ood, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mnested_custom_collate)\n\u001b[1;32m     12\u001b[0m val_dataset_16 \u001b[38;5;241m=\u001b[39m HDF5Dataset(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, alg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterp_data_16_eval.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m), nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/UniWork/GDL/NAR_project/clrs/interp/dataset.py:202\u001b[0m, in \u001b[0;36mHDF5Dataset.__init__\u001b[0;34m(self, filename, nested)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Open the file lazily\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnested \u001b[38;5;241m=\u001b[39m nested\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# Open temp to get len\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nested:\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;66;03m# For nested structure, count algorithms\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatapoint_0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/anaconda3/envs/gdl_env/lib/python3.11/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/anaconda3/envs/gdl_env/lib/python3.11/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'data/bellman_ford_bfs/interp_data_OOD_eval.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from interp.models import DummyJointModel\n",
    "from interp.train import evaluate_joint\n",
    "from interp.dataset import HDF5Dataset, nested_custom_collate\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "alg = \"bellman_ford_bfs\"\n",
    "data_source = os.path.join(\"data\", alg)\n",
    "\n",
    "val_dataset_ood = HDF5Dataset(os.path.join(\"data\", alg, \"interp_data_OOD_eval.h5\"), nested=True)\n",
    "val_dataloader_ood = DataLoader(val_dataset_ood, batch_size=4, shuffle=True, collate_fn=nested_custom_collate)\n",
    "val_dataset_16 = HDF5Dataset(os.path.join(\"data\", alg, \"interp_data_16_eval.h5\"), nested=True)\n",
    "val_dataloader_16 = DataLoader(val_dataset_16, batch_size=4, shuffle=True, collate_fn=nested_custom_collate)\n",
    "val_dataset_8 = HDF5Dataset(os.path.join(\"data\", alg, \"interp_data_8_eval.h5\"), nested=True)\n",
    "val_dataloader_8 = DataLoader(val_dataset_8, batch_size=4, shuffle=True, collate_fn=nested_custom_collate) \n",
    "val_dataset_all = HDF5Dataset(os.path.join(\"data\", alg, \"interp_data_all_eval.h5\"), nested=True)\n",
    "val_dataloader_all = DataLoader(val_dataset_all, batch_size=4, shuffle=True, collate_fn=nested_custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5a4f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing evaluation on OOD dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_dataloader_ood' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m m \u001b[38;5;241m=\u001b[39m DummyJointModel()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperforming evaluation on OOD dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluate_joint(m, val_dataloader_ood, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperforming evaluation on 16 dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluate_joint(m, val_dataloader_16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataloader_ood' is not defined"
     ]
    }
   ],
   "source": [
    "m = DummyJointModel()\n",
    "print(\"performing evaluation on OOD dataset\")\n",
    "print(evaluate_joint(m, val_dataloader_ood, \"cpu\"))\n",
    "print(\"performing evaluation on 16 dataset\")\n",
    "print(evaluate_joint(m, val_dataloader_16, \"cpu\"))\n",
    "print(\"performing evaluation on 8 dataset\")\n",
    "print(evaluate_joint(m, val_dataloader_8, \"cpu\"))\n",
    "print(\"performing evaluation on all dataset\")\n",
    "print(evaluate_joint(m, val_dataloader_all, \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb646011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing evaluation on OOD dataset\n",
      "(0.2754781371853604, {'bellman_ford': 0.1884538551209388, 'bfs': 0.362502419249782})\n",
      "performing evaluation on 16 dataset\n",
      "(0.0838113730271028, {'bellman_ford': 0.059216358357282874, 'bfs': 0.10840638769692272})\n",
      "performing evaluation on 8 dataset\n",
      "(0.05049937276329627, {'bellman_ford': 0.02702308467274604, 'bfs': 0.07397566085384649})\n",
      "performing evaluation on all dataset\n",
      "(0.06406357517081746, {'bellman_ford': 0.04593573106014741, 'bfs': 0.08219141928148752})\n"
     ]
    }
   ],
   "source": [
    "from interp.config import create_model_from_config, load_config\n",
    "import torch\n",
    "\n",
    "model_name = \"gnn_small_all\"\n",
    "checkpoint_source = os.path.join(\"interp_checkpoints\", alg, model_name)\n",
    "\n",
    "config = load_config(os.path.join(checkpoint_source, model_name + \"_config.json\"))\n",
    "model = create_model_from_config(config)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_source, model_name + \".pth\")))\n",
    "model.eval()\n",
    "\n",
    "print(\"performing evaluation on OOD dataset\")\n",
    "print(evaluate_joint(model, val_dataloader_ood, \"cpu\"))\n",
    "print(\"performing evaluation on 16 dataset\")\n",
    "print(evaluate_joint(model, val_dataloader_16, \"cpu\"))\n",
    "print(\"performing evaluation on 8 dataset\")\n",
    "print(evaluate_joint(model, val_dataloader_8, \"cpu\"))\n",
    "print(\"performing evaluation on all dataset\")\n",
    "print(evaluate_joint(model, val_dataloader_all, \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ce2ea",
   "metadata": {},
   "source": [
    "# Single algorithm training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fdfe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.models import DummyModel\n",
    "from interp.train import evaluate\n",
    "from interp.dataset import HDF5Dataset, custom_collate\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "alg = \"bfs\"\n",
    "data_source = os.path.join(\"data\", alg)\n",
    "\n",
    "val_dataset_ood = HDF5Dataset(os.path.join(\"data\", alg, \"interp_data_OOD_eval.h5\"), nested=False)\n",
    "val_dataloader_ood = DataLoader(val_dataset_ood, batch_size=4, shuffle=True, collate_fn=custom_collate)\n",
    "val_dataset_16 = HDF5Dataset(os.path.join(\"data\", alg, \"interp_data_16_eval.h5\"), nested=False)\n",
    "val_dataloader_16 = DataLoader(val_dataset_16, batch_size=4, shuffle=True, collate_fn=custom_collate)\n",
    "val_dataset_8 = HDF5Dataset(os.path.join(\"data\", alg, \"interp_data_8_eval.h5\"), nested=False)\n",
    "val_dataloader_8 = DataLoader(val_dataset_8, batch_size=4, shuffle=True, collate_fn=custom_collate) \n",
    "val_dataset_all = HDF5Dataset(os.path.join(\"data\", alg, \"interp_data_all_eval.h5\"), nested=False)\n",
    "val_dataloader_all = DataLoader(val_dataset_all, batch_size=4, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1fb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing evaluation on OOD dataset\n",
      "4.378232815206768\n",
      "performing evaluation on 16 dataset\n",
      "2.641170819009547\n",
      "performing evaluation on 8 dataset\n",
      "1.8725684665205624\n",
      "performing evaluation on all dataset\n",
      "2.252141742611517\n"
     ]
    }
   ],
   "source": [
    "m = DummyModel()\n",
    "print(\"performing evaluation on OOD dataset\")\n",
    "print(evaluate(m, val_dataloader_ood, \"cpu\"))\n",
    "print(\"performing evaluation on 16 dataset\")\n",
    "print(evaluate(m, val_dataloader_16, \"cpu\"))\n",
    "print(\"performing evaluation on 8 dataset\")\n",
    "print(evaluate(m, val_dataloader_8, \"cpu\"))\n",
    "print(\"performing evaluation on all dataset\")\n",
    "print(evaluate(m, val_dataloader_all, \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625249e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing evaluation on OOD dataset\n",
      "0.35293075903315874\n",
      "performing evaluation on 16 dataset\n",
      "0.10563356048451542\n",
      "performing evaluation on 8 dataset\n",
      "0.0758527074026126\n",
      "performing evaluation on all dataset\n",
      "0.08376356629893539\n"
     ]
    }
   ],
   "source": [
    "from interp.config import create_model_from_config, load_config\n",
    "import torch\n",
    "\n",
    "model_name = \"gnn_small_all\"\n",
    "checkpoint_source = os.path.join(\"interp_checkpoints\", alg, model_name)\n",
    "\n",
    "config = load_config(os.path.join(checkpoint_source, model_name + \"_config.json\"))\n",
    "model = create_model_from_config(config)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_source, model_name + \".pth\")))\n",
    "model.eval()\n",
    "\n",
    "print(\"performing evaluation on OOD dataset\")\n",
    "print(evaluate(model, val_dataloader_ood, \"cpu\"))\n",
    "print(\"performing evaluation on 16 dataset\")\n",
    "print(evaluate(model, val_dataloader_16, \"cpu\"))\n",
    "print(\"performing evaluation on 8 dataset\")\n",
    "print(evaluate(model, val_dataloader_8, \"cpu\"))\n",
    "print(\"performing evaluation on all dataset\")\n",
    "print(evaluate(model, val_dataloader_all, \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684b5aa",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f974f3-2c77-48b9-93f6-3cca169de204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.models import GNNJointInterpNetwork\n",
    "from interp.evaluation import evaluate_model_on_dataset, visualize_results\n",
    "\n",
    "# Load a trained joint model\n",
    "model = GNNJointInterpNetwork(\n",
    "    hidden_dim=128, \n",
    "    algorithms=[\"bellman_ford\", \"bfs\"]\n",
    ")\n",
    "model.load_state_dict(torch.load(\"path/to/joint_model.pth\"))\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model_on_dataset(\n",
    "    model, \n",
    "    dataset_path=\"data/bellman_ford_bfs/interp_data_16_eval.h5\",\n",
    "    batch_size=16,\n",
    "    nested=True\n",
    ")\n",
    "\n",
    "# Visualize the results\n",
    "visualize_results(results, title=\"Joint Model Evaluation\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
