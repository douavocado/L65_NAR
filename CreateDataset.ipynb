{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ef501b-c355-4825-af5b-20c2bc07b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 23:56:25.977648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741391786.034375   49810 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741391786.049648   49810 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import clrs\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "encode_hints = True\n",
    "decode_hints = True\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2**32, dtype=np.int64))\n",
    "\n",
    "processor_factory = clrs.get_processor_factory(\n",
    "    'triplet_gmpnn',\n",
    "    use_ln=True,\n",
    "    nb_triplet_fts=8,\n",
    "    nb_heads=1,\n",
    "    )\n",
    "model_params = dict(\n",
    "    processor_factory=processor_factory,\n",
    "    hidden_dim=128,\n",
    "    encode_hints=encode_hints,\n",
    "    decode_hints=decode_hints,\n",
    "    encoder_init='xavier_on_scalars',\n",
    "    use_lstm=False,\n",
    "    learning_rate=0.001,\n",
    "    grad_clip_max_norm=1.0,\n",
    "    checkpoint_path='checkpoints/CLRS30',\n",
    "    freeze_processor=False,\n",
    "    dropout_prob=0.0,\n",
    "    hint_teacher_forcing=0.0,\n",
    "    hint_repred_mode='soft',\n",
    "    nb_msg_passing_steps=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aabfa4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from interp.dataset import HDF5Dataset\n",
    "\n",
    "# --- Saving Data ---\n",
    "def save_to_hdf5(data, filename, nested=True):\n",
    "    \"\"\"\n",
    "    Save data to HDF5 file format.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries (or dictionary of dictionaries if nested=True)\n",
    "        filename: Path to save the HDF5 file\n",
    "        nested: If True, treats data as list of dictionary of dictionaries.\n",
    "                If False, treats data as list of dictionaries.\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        for i, datapoint in enumerate(data):\n",
    "            group = f.create_group(f'datapoint_{i}')  # Create a group for each datapoint\n",
    "            \n",
    "            if nested:\n",
    "                # Handle nested dictionary structure\n",
    "                for algo_key, algo_dict in datapoint.items():\n",
    "                    # Create a subgroup for each algorithm\n",
    "                    algo_group = group.create_group(algo_key)\n",
    "                    # Store each array in the algorithm's dictionary\n",
    "                    for key, array in algo_dict.items():\n",
    "                        algo_group.create_dataset(key, data=array, compression=\"gzip\")\n",
    "            else:\n",
    "                # Handle flat dictionary structure\n",
    "                for key, array in datapoint.items():\n",
    "                    # Store each array as a dataset within the group\n",
    "                    group.create_dataset(key, data=array, compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d63074",
   "metadata": {},
   "source": [
    "# Creating joint training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8e7fa",
   "metadata": {},
   "source": [
    "Only run these cells if creating training data where multiple algorithms are being trained on the interpretation models at once. Otherwise skip to the next section for single algorithm data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701b89df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3883.18it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 5067.97it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1494.97it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2075.64it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 500.55it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 738.33it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 345.12it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 570.26it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 183.03it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 419.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# For JOINT Algorithm sampling data creation\n",
    "# each datapoint is a dictionary of dictionaries. The outer dictionary has keys as the algorithm names and the inner dictionary has keys as the algorithm names and the values are the dataloader for that algorithm.\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "# Set up multiple lengths and samples per length\n",
    "# LENGTHS = [16]\n",
    "LENGTHS = [4, 7, 11, 13, 16]\n",
    "# LENGTHS = [20, 25, 30, 35, 40, 45, 50, 55, 60, 64]\n",
    "SAMPLES_PER_LENGTH = 1000\n",
    "\n",
    "algorithms = [\"bellman_ford\", \"bfs\"]\n",
    "\n",
    "data = {}\n",
    "for length_idx, length in enumerate(LENGTHS):\n",
    "    new_rng_key, rng_key = jax.random.split(rng_key)    \n",
    "\n",
    "    # Create data for all samples of this length for each algorithm\n",
    "    for algo in algorithms:\n",
    "        # Create sampler for this length\n",
    "        sampler, spec = clrs.build_sampler(\n",
    "            algo,\n",
    "            seed=rng.randint(2**32, dtype=np.int64),\n",
    "            num_samples=SAMPLES_PER_LENGTH,\n",
    "            length=length,\n",
    "        )\n",
    "\n",
    "        # Get dummy trajectory and initialize model\n",
    "        dummy_traj = [sampler.next()]\n",
    "        model = clrs.models.BaselineModel(\n",
    "            spec=[spec],\n",
    "            dummy_trajectory=dummy_traj,\n",
    "            get_inter=True,\n",
    "            **model_params\n",
    "        )\n",
    "\n",
    "        all_features = [f.features for f in dummy_traj]\n",
    "        model.init(all_features, 42)\n",
    "\n",
    "        # Get predictions for this length\n",
    "        feedback = sampler.next()\n",
    "\n",
    "        model.restore_model(f'best_{algo}.pkl', only_load_processor=False)\n",
    "        preds, _, hist = model.predict(new_rng_key, feedback.features)\n",
    "        for item in tqdm(range(SAMPLES_PER_LENGTH)):\n",
    "            \n",
    "            feedback_hint_names = [f.name for f in feedback.features.hints]\n",
    "            feedback_input_names = [f.name for f in feedback.features.inputs]\n",
    "            feedback_output_names = [f.name for f in feedback.outputs]\n",
    "\n",
    "            graph_adj = feedback.features.inputs[feedback_input_names.index(\"adj\")].data[item] # (D, D)\n",
    "            edge_weights = feedback.features.inputs[feedback_input_names.index(\"A\")].data[item] # (D, D)\n",
    "            start_node = feedback.features.inputs[feedback_input_names.index(\"s\")].data[item] # (D)\n",
    "            \n",
    "            gt_pi = feedback.outputs[feedback_output_names.index(\"pi\")].data[item] # (D)\n",
    "\n",
    "            # for upd_pi, sometimes we need to get rid of trailing zero entries (for algorithms that don't have fixed numbre of time steps)\n",
    "            # detect whether there are trailing zero vectors in the last time dimensions of upd_pi and find the cutoff index\n",
    "            # then take the first n entries of upd_pi and upd_d where n is the cutoff index\n",
    "            \n",
    "            raw_upd_pi = feedback.features.hints[feedback_hint_names.index(\"upd_pi\")].data[:,item,:] # (T, D)\n",
    "            raw_upd_d = feedback.features.hints[feedback_hint_names.index(\"upd_d\")].data[:,item,:] # (T, D)\n",
    "            \n",
    "            # Find the cutoff index by detecting trailing zero vectors\n",
    "            # A vector is considered zero if all its elements are zero\n",
    "            is_zero_vector = np.all(raw_upd_pi == np.zeros(raw_upd_pi.shape[1]), axis=1)\n",
    "            \n",
    "            # Find the last non-zero vector index\n",
    "            non_zero_indices = np.where(~is_zero_vector)[0]\n",
    "            if len(non_zero_indices) > 0:\n",
    "                cutoff_idx = non_zero_indices[-1] + 1  # +1 to include the last non-zero vector\n",
    "            else:\n",
    "                cutoff_idx = raw_upd_pi.shape[0]  # Use all if no zero vectors found\n",
    "            \n",
    "            # if cutoff_idx is less than 2, we should make upd_pi and upd_d have length 2, but pad upd_pi with arange(length) and upd_d with zeros\n",
    "            if cutoff_idx < 2:\n",
    "                upd_pi = raw_upd_pi[:2]\n",
    "                upd_d = raw_upd_d[:2]\n",
    "                for i in range(2-cutoff_idx):\n",
    "                    upd_pi[-i-1] = np.arange(raw_upd_pi.shape[1])\n",
    "                    upd_d[-i-1] = np.zeros(raw_upd_d.shape[1], dtype=np.float32)\n",
    "                cutoff_idx = 2\n",
    "            else:\n",
    "                # Take only the first n entries where n is the cutoff index\n",
    "                upd_pi = raw_upd_pi[:cutoff_idx]\n",
    "                upd_d = raw_upd_d[:cutoff_idx]\n",
    "\n",
    "            hidden_states = np.stack([hist[i].hiddens[item] for i in range(cutoff_idx)]).transpose((0,2,1)) # (T, H, D)\n",
    "            \n",
    "            datapoint = {\n",
    "                'hidden_states': deepcopy(hidden_states),\n",
    "                'graph_adj': deepcopy(graph_adj),\n",
    "                'edge_weights': deepcopy(edge_weights), \n",
    "                'upd_pi': deepcopy(upd_pi),\n",
    "                'upd_d': deepcopy(upd_d),\n",
    "                'gt_pi': deepcopy(gt_pi),\n",
    "                'start_node': deepcopy(start_node),\n",
    "            }\n",
    "\n",
    "            if item + length_idx*SAMPLES_PER_LENGTH not in data:\n",
    "                data[item + length_idx*SAMPLES_PER_LENGTH] = {}  \n",
    "            \n",
    "            data[item + length_idx*SAMPLES_PER_LENGTH][algo] = deepcopy(datapoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d107098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "train_data, val_data = train_test_split(data, train_size=TRAIN_RATIO, random_state=42)\n",
    "\n",
    "save_root = os.path.join(\"data\", \"_\".join(algorithms))\n",
    "save_name = \"interp_data_all\"\n",
    "\n",
    "save_to_hdf5(train_data, os.path.join(save_root, save_name + \".h5\"), nested=True)\n",
    "save_to_hdf5(val_data, os.path.join(save_root, save_name + \"_eval.h5\"), nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "108412a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 4500\n",
      "Validation dataset size: 500\n",
      "Sample batch keys: dict_keys(['bellman_ford', 'bfs'])\n",
      "algo bellman_ford batch torch.Size([64])\n",
      "algo bellman_ford num_graphs torch.Size([])\n",
      "algo bellman_ford num_nodes_per_graph torch.Size([4])\n",
      "algo bellman_ford all_cumsum torch.Size([5])\n",
      "algo bellman_ford edge_weights torch.Size([64, 64])\n",
      "algo bellman_ford graph_adj torch.Size([64, 64])\n",
      "algo bellman_ford gt_pi torch.Size([64])\n",
      "algo bellman_ford hidden_states torch.Size([64, 128, 64])\n",
      "algo bellman_ford timesteps_per_graph torch.Size([4])\n",
      "algo bellman_ford all_cumsum_timesteps torch.Size([5])\n",
      "algo bellman_ford start_node torch.Size([64])\n",
      "algo bellman_ford upd_d torch.Size([64, 64])\n",
      "algo bellman_ford upd_pi torch.Size([64, 64])\n",
      "algo bfs batch torch.Size([64])\n",
      "algo bfs num_graphs torch.Size([])\n",
      "algo bfs num_nodes_per_graph torch.Size([4])\n",
      "algo bfs all_cumsum torch.Size([5])\n",
      "algo bfs edge_weights torch.Size([64, 64])\n",
      "algo bfs graph_adj torch.Size([64, 64])\n",
      "algo bfs gt_pi torch.Size([64])\n",
      "algo bfs hidden_states torch.Size([15, 128, 64])\n",
      "algo bfs timesteps_per_graph torch.Size([4])\n",
      "algo bfs all_cumsum_timesteps torch.Size([5])\n",
      "algo bfs start_node torch.Size([64])\n",
      "algo bfs upd_d torch.Size([15, 64])\n",
      "algo bfs upd_pi torch.Size([15, 64])\n"
     ]
    }
   ],
   "source": [
    "from interp.dataset import HDF5Dataset\n",
    "from interp.dataset import custom_collate, nested_custom_collate\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "# algorithms = [\"bellman_ford\", \"bfs\"]\n",
    "# save_root = os.path.join(\"data\", \"_\".join(algorithms))\n",
    "# save_name = \"interp_data_all\"\n",
    "\n",
    "# Load the saved datasets\n",
    "train_dataset = HDF5Dataset(os.path.join(save_root, save_name + \".h5\"), nested=True)\n",
    "val_dataset = HDF5Dataset(os.path.join(save_root, save_name + \"_eval.h5\"), nested=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=nested_custom_collate\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=nested_custom_collate\n",
    ")\n",
    "\n",
    "# Print some information about the datasets\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Check the first batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Sample batch keys:\", sample_batch.keys())\n",
    "\n",
    "for key in sample_batch.keys():\n",
    "    algo_input = sample_batch[key]\n",
    "    for field in algo_input.keys():\n",
    "        print(\"algo\", key, field, algo_input[field].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeccb1b",
   "metadata": {},
   "source": [
    "# Creating single algorithm Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd4818",
   "metadata": {},
   "source": [
    "Use this section code for single algorithm data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c7cdcf-5656-4da7-a82c-42d96ec0388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 100.09it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 49.10it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 25.80it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.35it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  9.85it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  6.89it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.79it/s]\n",
      "100%|██████████| 20/20 [00:05<00:00,  3.44it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.57it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Set up multiple lengths and samples per length\n",
    "# LENGTHS = [16]\n",
    "# LENGTHS = [4, 7, 11, 13, 16]\n",
    "LENGTHS = [20, 25, 30, 35, 40, 45, 50, 55, 60, 64]\n",
    "SAMPLES_PER_LENGTH = 20\n",
    "\n",
    "alg = \"dijkstra\"\n",
    "\n",
    "data = []\n",
    "for length in LENGTHS:\n",
    "    # Create sampler for this length\n",
    "    sampler, spec = clrs.build_sampler(\n",
    "        alg,\n",
    "        seed=rng.randint(2**32, dtype=np.int64),\n",
    "        num_samples=SAMPLES_PER_LENGTH,\n",
    "        length=length,\n",
    "    )\n",
    "\n",
    "    # Get dummy trajectory and initialize model\n",
    "    dummy_traj = [sampler.next()]\n",
    "    model = clrs.models.BaselineModel(\n",
    "        spec=[spec],\n",
    "        dummy_trajectory=dummy_traj,\n",
    "        get_inter=True,\n",
    "        **model_params\n",
    "    )\n",
    "\n",
    "    all_features = [f.features for f in dummy_traj]\n",
    "    model.init(all_features, 42)\n",
    "    model.restore_model(f'best_{alg}.pkl', only_load_processor=False)\n",
    "\n",
    "    # Get predictions for this length\n",
    "    feedback = sampler.next()\n",
    "    new_rng_key, rng_key = jax.random.split(rng_key)\n",
    "    preds, _, hist = model.predict(new_rng_key, feedback.features)\n",
    "\n",
    "    # Create data for all samples of this length\n",
    "    for item in tqdm(range(SAMPLES_PER_LENGTH)):\n",
    "        hidden_states = np.stack([hist[i].hiddens[item] for i in range(len(hist))]).transpose((0,2,1))\n",
    "        feedback_hint_names = [f.name for f in feedback.features.hints]\n",
    "        feedback_input_names = [f.name for f in feedback.features.inputs]\n",
    "        feedback_output_names = [f.name for f in feedback.outputs]\n",
    "\n",
    "        graph_adj = feedback.features.inputs[feedback_input_names.index(\"adj\")].data[item] # (D, D)\n",
    "        edge_weights = feedback.features.inputs[feedback_input_names.index(\"A\")].data[item] # (D, D)\n",
    "        start_node = feedback.features.inputs[feedback_input_names.index(\"s\")].data[item] # (D)\n",
    "        upd_pi = feedback.features.hints[feedback_hint_names.index(\"upd_pi\")].data[:,item,:] # (T, D)\n",
    "        upd_d = feedback.features.hints[feedback_hint_names.index(\"upd_d\")].data[:,item,:] # (T, D)\n",
    "        gt_pi = feedback.outputs[feedback_output_names.index(\"pi\")].data[item] # (D)\n",
    "        \n",
    "        datapoint = {\n",
    "            'hidden_states': np.copy(hidden_states),\n",
    "            'graph_adj': np.copy(graph_adj),\n",
    "            'edge_weights': np.copy(edge_weights), \n",
    "            'upd_pi': np.copy(upd_pi),\n",
    "            'upd_d': np.copy(upd_d),\n",
    "            'gt_pi': np.copy(gt_pi),\n",
    "            'start_node': np.copy(start_node),\n",
    "        }\n",
    "        data.append(datapoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ce68ac-9b48-43fa-8036-c942f8bec418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 128, 45)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "data[random.randint(0, len(data)-1)][\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b4daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_hdf5(data, \"data/dijkstra/interp_data_OOD_20_64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c3f15e-cb1c-48fd-ad25-05753c466794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.dataset import custom_collate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "train_data, val_data = train_test_split(data, train_size=TRAIN_RATIO, random_state=42)\n",
    "\n",
    "save_root = os.path.join(\"data\", alg)\n",
    "save_name = \"interp_data_16\"\n",
    "\n",
    "save_to_hdf5(train_data, os.path.join(save_root, save_name + \".h5\"))\n",
    "save_to_hdf5(val_data, os.path.join(save_root, save_name + \"_eval.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb52193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shapes after collation:\n",
      "batch: torch.Size([64])\n",
      "num_graphs: torch.Size([])\n",
      "num_nodes_per_graph: torch.Size([4])\n",
      "all_cumsum: torch.Size([5])\n",
      "edge_weights: torch.Size([64, 64])\n",
      "graph_adj: torch.Size([64, 64])\n",
      "gt_pi: torch.Size([64])\n",
      "hidden_states: torch.Size([68, 128, 64])\n",
      "timesteps_per_graph: torch.Size([4])\n",
      "all_cumsum_timesteps: torch.Size([5])\n",
      "start_node: torch.Size([64])\n",
      "upd_d: torch.Size([68, 64])\n",
      "upd_pi: torch.Size([68, 64])\n"
     ]
    }
   ],
   "source": [
    "# Load the saved dataset and check shapes after collation\n",
    "import os\n",
    "from interp.dataset import custom_collate\n",
    "from torch.utils.data import DataLoader\n",
    "from interp.dataset import HDF5Dataset\n",
    "\n",
    "alg = \"dijkstra\"\n",
    "save_root = os.path.join(\"data\", alg)\n",
    "save_name = \"interp_data_16\"\n",
    "\n",
    "# Load the saved datasets\n",
    "train_dataset = HDF5Dataset(os.path.join(save_root, save_name + \".h5\"))\n",
    "val_dataset = HDF5Dataset(os.path.join(save_root, save_name + \"_eval.h5\"))\n",
    "\n",
    "# Create dataloaders with the custom_collate function\n",
    "batch_size = 4  # Small batch size for demonstration\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "# Get a sample batch from the dataloader\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Print the shapes of each entry in the batch\n",
    "print(\"Sample batch shapes after collation:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"{key}: list of length {len(value)}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)}\")\n",
    "\n",
    "# Close the HDF5 files when done\n",
    "train_dataset.close()\n",
    "val_dataset.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0c89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
