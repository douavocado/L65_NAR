{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ef501b-c355-4825-af5b-20c2bc07b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 14:26:26.048411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741271186.061597   35588 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741271186.064746   35588 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import os\n",
    "import shutil\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import clrs\n",
    "import jax\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def visualize_graph_from_adjacency_matrix(adjacency_matrix, weight_matrix=None):\n",
    "    \"\"\"\n",
    "    Visualizes a graph with explicit arrows and labeled edge weights (adjacent).\n",
    "\n",
    "    Args:\n",
    "        adjacency_matrix: Adjacency matrix (NumPy array).\n",
    "        weight_matrix: Optional weight matrix (NumPy array).\n",
    "    \"\"\"\n",
    "\n",
    "    adjacency_matrix = np.array(adjacency_matrix)\n",
    "    if adjacency_matrix.shape[0] != adjacency_matrix.shape[1]:\n",
    "        raise ValueError(\"Adjacency matrix must be square.\")\n",
    "    num_nodes = adjacency_matrix.shape[0]\n",
    "\n",
    "    if weight_matrix is None:\n",
    "        weight_matrix = np.ones_like(adjacency_matrix)\n",
    "    else:\n",
    "        weight_matrix = np.array(weight_matrix)\n",
    "        if weight_matrix.shape != adjacency_matrix.shape:\n",
    "            raise ValueError(\"Weight matrix must have the same dimensions.\")\n",
    "\n",
    "    directed_graph = nx.DiGraph()\n",
    "    undirected_graph = nx.Graph()\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        directed_graph.add_node(i)\n",
    "        undirected_graph.add_node(i)\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if adjacency_matrix[i, j] != 0:\n",
    "                    weight = round(weight_matrix[i, j], 2)\n",
    "                    if adjacency_matrix[j, i] != 0:\n",
    "                        if i < j:\n",
    "                            undirected_graph.add_edge(i, j, weight=weight)\n",
    "                    else:\n",
    "                        directed_graph.add_edge(i, j, weight=weight)\n",
    "\n",
    "    pos = nx.spring_layout(undirected_graph)  # Layout based on undirected\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Draw undirected edges (no arrows)\n",
    "    nx.draw_networkx_edges(undirected_graph, pos, edge_color='gray', width=2, arrows=False)\n",
    "    edge_labels_undirected = nx.get_edge_attributes(undirected_graph, 'weight')\n",
    "    # Use label_pos and rotate for adjacent labels\n",
    "    nx.draw_networkx_edge_labels(undirected_graph, pos, edge_labels=edge_labels_undirected,\n",
    "                                 label_pos=0.3, rotate=True)\n",
    "\n",
    "    # Draw directed edges with explicit arrows\n",
    "    nx.draw_networkx_edges(directed_graph, pos, edge_color='black', width=1,\n",
    "                           arrowstyle='->', arrowsize=15)\n",
    "    edge_labels_directed = nx.get_edge_attributes(directed_graph, 'weight')\n",
    "    # Use label_pos and rotate for adjacent labels\n",
    "    nx.draw_networkx_edge_labels(directed_graph, pos, edge_labels=edge_labels_directed,\n",
    "                                 label_pos=0.3, rotate=True)\n",
    "\n",
    "    nx.draw_networkx_nodes(directed_graph, pos, node_color='skyblue', node_size=500)\n",
    "    nx.draw_networkx_labels(directed_graph, pos)\n",
    "\n",
    "    plt.title(\"Graph Visualization\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "encode_hints = True\n",
    "decode_hints = True\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2**32, dtype=np.int64))\n",
    "\n",
    "processor_factory = clrs.get_processor_factory(\n",
    "    'triplet_gmpnn',\n",
    "    use_ln=True,\n",
    "    nb_triplet_fts=8,\n",
    "    nb_heads=1,\n",
    "    )\n",
    "model_params = dict(\n",
    "    processor_factory=processor_factory,\n",
    "    hidden_dim=128,\n",
    "    encode_hints=encode_hints,\n",
    "    decode_hints=decode_hints,\n",
    "    encoder_init='xavier_on_scalars',\n",
    "    use_lstm=False,\n",
    "    learning_rate=0.001,\n",
    "    grad_clip_max_norm=1.0,\n",
    "    checkpoint_path='checkpoints/CLRS30',\n",
    "    freeze_processor=False,\n",
    "    dropout_prob=0.0,\n",
    "    hint_teacher_forcing=0.0,\n",
    "    hint_repred_mode='soft',\n",
    "    nb_msg_passing_steps=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09612d5b-40e8-4fee-bd78-a64a20df5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from interp.dataset import HDF5Dataset\n",
    "\n",
    "# --- Saving Data ---\n",
    "def save_to_hdf5(data, filename):\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        for i, datapoint in enumerate(data):\n",
    "            group = f.create_group(f'datapoint_{i}')  # Create a group for each datapoint\n",
    "            for key, array in datapoint.items():\n",
    "                # Store each array as a dataset within the group\n",
    "                group.create_dataset(key, data=array, compression=\"gzip\") # Optional compression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c7cdcf-5656-4da7-a82c-42d96ec0388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 100.09it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 49.10it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 25.80it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.35it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  9.85it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  6.89it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.79it/s]\n",
      "100%|██████████| 20/20 [00:05<00:00,  3.44it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.57it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Set up multiple lengths and samples per length\n",
    "# LENGTHS = [16]\n",
    "# LENGTHS = [4, 7, 11, 13, 16]\n",
    "LENGTHS = [20, 25, 30, 35, 40, 45, 50, 55, 60, 64]\n",
    "SAMPLES_PER_LENGTH = 20\n",
    "\n",
    "alg = \"dijkstra\"\n",
    "\n",
    "data = []\n",
    "for length in LENGTHS:\n",
    "    # Create sampler for this length\n",
    "    sampler, spec = clrs.build_sampler(\n",
    "        alg,\n",
    "        seed=rng.randint(2**32, dtype=np.int64),\n",
    "        num_samples=SAMPLES_PER_LENGTH,\n",
    "        length=length,\n",
    "    )\n",
    "\n",
    "    # Get dummy trajectory and initialize model\n",
    "    dummy_traj = [sampler.next()]\n",
    "    model = clrs.models.BaselineModel(\n",
    "        spec=[spec],\n",
    "        dummy_trajectory=dummy_traj,\n",
    "        get_inter=True,\n",
    "        **model_params\n",
    "    )\n",
    "\n",
    "    all_features = [f.features for f in dummy_traj]\n",
    "    model.init(all_features, 42)\n",
    "    model.restore_model(f'best_{alg}.pkl', only_load_processor=False)\n",
    "\n",
    "    # Get predictions for this length\n",
    "    feedback = sampler.next()\n",
    "    new_rng_key, rng_key = jax.random.split(rng_key)\n",
    "    preds, _, hist = model.predict(new_rng_key, feedback.features)\n",
    "\n",
    "    # Create data for all samples of this length\n",
    "    for item in tqdm(range(SAMPLES_PER_LENGTH)):\n",
    "        hidden_states = np.stack([hist[i].hiddens[item] for i in range(len(hist))]).transpose((0,2,1))\n",
    "        feedback_hint_names = [f.name for f in feedback.features.hints]\n",
    "        feedback_input_names = [f.name for f in feedback.features.inputs]\n",
    "        feedback_output_names = [f.name for f in feedback.outputs]\n",
    "\n",
    "        graph_adj = feedback.features.inputs[feedback_input_names.index(\"adj\")].data[item] # (D, D)\n",
    "        edge_weights = feedback.features.inputs[feedback_input_names.index(\"A\")].data[item] # (D, D)\n",
    "        start_node = feedback.features.inputs[feedback_input_names.index(\"s\")].data[item] # (D)\n",
    "        upd_pi = feedback.features.hints[feedback_hint_names.index(\"upd_pi\")].data[:,item,:] # (T, D)\n",
    "        upd_d = feedback.features.hints[feedback_hint_names.index(\"upd_d\")].data[:,item,:] # (T, D)\n",
    "        gt_pi = feedback.outputs[feedback_output_names.index(\"pi\")].data[item] # (D)\n",
    "        \n",
    "        datapoint = {\n",
    "            'hidden_states': np.copy(hidden_states),\n",
    "            'graph_adj': np.copy(graph_adj),\n",
    "            'edge_weights': np.copy(edge_weights), \n",
    "            'upd_pi': np.copy(upd_pi),\n",
    "            'upd_d': np.copy(upd_d),\n",
    "            'gt_pi': np.copy(gt_pi),\n",
    "            'start_node': np.copy(start_node),\n",
    "        }\n",
    "        data.append(datapoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ce68ac-9b48-43fa-8036-c942f8bec418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 128, 45)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "data[random.randint(0, len(data)-1)][\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b4daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_hdf5(data, \"data/dijkstra/interp_data_OOD_20_64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c3f15e-cb1c-48fd-ad25-05753c466794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.dataset import custom_collate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "train_data, val_data = train_test_split(data, train_size=TRAIN_RATIO, random_state=42)\n",
    "\n",
    "save_root = os.path.join(\"data\", alg)\n",
    "save_name = \"interp_data_16\"\n",
    "\n",
    "save_to_hdf5(train_data, os.path.join(save_root, save_name + \".h5\"))\n",
    "save_to_hdf5(val_data, os.path.join(save_root, save_name + \"_eval.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb52193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shapes after collation:\n",
      "batch: torch.Size([64])\n",
      "num_graphs: torch.Size([])\n",
      "num_nodes_per_graph: torch.Size([4])\n",
      "all_cumsum: torch.Size([5])\n",
      "edge_weights: torch.Size([64, 64])\n",
      "graph_adj: torch.Size([64, 64])\n",
      "gt_pi: torch.Size([64])\n",
      "hidden_states: torch.Size([68, 128, 64])\n",
      "timesteps_per_graph: torch.Size([4])\n",
      "all_cumsum_timesteps: torch.Size([5])\n",
      "start_node: torch.Size([64])\n",
      "upd_d: torch.Size([68, 64])\n",
      "upd_pi: torch.Size([68, 64])\n"
     ]
    }
   ],
   "source": [
    "# Load the saved dataset and check shapes after collation\n",
    "import os\n",
    "from interp.dataset import custom_collate\n",
    "from torch.utils.data import DataLoader\n",
    "from interp.dataset import HDF5Dataset\n",
    "\n",
    "alg = \"dijkstra\"\n",
    "save_root = os.path.join(\"data\", alg)\n",
    "save_name = \"interp_data_16\"\n",
    "\n",
    "# Load the saved datasets\n",
    "train_dataset = HDF5Dataset(os.path.join(save_root, save_name + \".h5\"))\n",
    "val_dataset = HDF5Dataset(os.path.join(save_root, save_name + \"_eval.h5\"))\n",
    "\n",
    "# Create dataloaders with the custom_collate function\n",
    "batch_size = 4  # Small batch size for demonstration\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "# Get a sample batch from the dataloader\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Print the shapes of each entry in the batch\n",
    "print(\"Sample batch shapes after collation:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"{key}: list of length {len(value)}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)}\")\n",
    "\n",
    "# Close the HDF5 files when done\n",
    "train_dataset.close()\n",
    "val_dataset.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ad1756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 17, 34, 51, 68])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch[\"all_cumsum_timesteps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0c89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
